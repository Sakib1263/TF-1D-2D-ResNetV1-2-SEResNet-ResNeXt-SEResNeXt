{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ResNet_Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFyb0y7PUJOo"
      },
      "source": [
        "# ResNet Model Building Pipeline for 1D Signals with DEMO\n",
        "#### ResNet18, ResNet34, ResNet50, ResNet101, ResNet152"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9vTr2NhcGAA"
      },
      "source": [
        "# Test GPU (Optional)\n",
        "Before Starting, kindly check the available GPU from the Google Server, GPU model and other related information. It might help!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUNJNtNxcFTF",
        "outputId": "a33a2ea6-e14e-4190-cacc-10ce346ba07f"
      },
      "source": [
        "import torch\n",
        "print(\"Is CUDA enabled GPU Available?\", torch.cuda.is_available())\n",
        "print(\"GPU Number:\", torch.cuda.device_count())\n",
        "print(\"Current GPU Index:\", torch.cuda.current_device())\n",
        "print(\"GPU Type:\", torch.cuda.get_device_name(device=None))\n",
        "print(\"GPU Capability:\", torch.cuda.get_device_capability(device=None))\n",
        "print(\"Is GPU Initialized yet?\", torch.cuda.is_initialized())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is CUDA enabled GPU Available? True\n",
            "GPU Number: 1\n",
            "Current GPU Index: 0\n",
            "GPU Type: Tesla P100-PCIE-16GB\n",
            "GPU Capability: (6, 0)\n",
            "Is GPU Initialized yet? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQbDnsIZUSx0"
      },
      "source": [
        "# Connect to Google Drive (Optional for Google COLAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2neoNCsqGgE"
      },
      "source": [
        "Copy-Paste the Authorization Code and Mount Google Drive to COLAB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg4kMbwxsH9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd4affa-e0a7-48cf-caa9-4a365fb3fcbb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/GDrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/GDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpdYZ2x3MbTd"
      },
      "source": [
        "Move to the Target Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efh543dRsQga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9602c498-796f-4b6e-b37a-111bab541d79"
      },
      "source": [
        "%cd /content/GDrive/MyDrive/Colab_Notebooks/GitHub/ResNet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/GDrive/MyDrive/Colab_Notebooks/GitHub/ResNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZtMdxE7MeXX"
      },
      "source": [
        "List the Files and Folders Located in the Current Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM92ZPcisMK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80bde54-2442-49f8-8569-29235a515a0e"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Drug_Persistency.xlsx\t   ResNet_1DCNN.py\t\t SelfResNet.py\n",
            "'Gender_H_W Dataset.csv'   ResNet_Implementation.ipynb\t'SOCR Dataset.csv'\n",
            " __pycache__\t\t   Saved_Model.h5\t\t VGG_1DCNN.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGqHvfZEnXAZ"
      },
      "source": [
        "Upload Files from Local Directory (if required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwDpRFYYnOYv"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgW7r0C9TuZk"
      },
      "source": [
        "#Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMhBhz1CrMb3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D, concatenate, BatchNormalization, Activation, add\n",
        "from keras.layers import Conv2D, MaxPooling2D, Reshape, Flatten, Dense\n",
        "from keras.models import Model, model_from_json\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "466PncP2BqnH"
      },
      "source": [
        "# Import ResNet1D Module\n",
        "from ResNet_1DCNN import ResNet"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL73Q1sxcL6S"
      },
      "source": [
        "# DEMO: Regression and Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRzYwYcMt7fF"
      },
      "source": [
        "## Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3rv02oyfds"
      },
      "source": [
        "### Import and Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_ijTp2G0qFo"
      },
      "source": [
        "Import Dataset from a CSV file using Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SybkeF4yjjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d863ff-32af-4c29-ab5b-03f7e92957be"
      },
      "source": [
        "dataset = pd.read_csv('Gender_H_W Dataset.csv')\n",
        "print(dataset.shape)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrWikf4A091c"
      },
      "source": [
        "Have a look at the dataset! The dataset contains Heights and Weights of 10000 people with their respective Gender. Either Height or Weight can be the target variable for Regression. The same dataset can also be used for Classification is someone wants to predict the gender based on respective Height and Weight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "vASo4FV0nZGM",
        "outputId": "52d0f230-a307-4e40-adb8-e7f23b65c6f5"
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>73.847017</td>\n",
              "      <td>241.893563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>68.781904</td>\n",
              "      <td>162.310473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>74.110105</td>\n",
              "      <td>212.740856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>71.730978</td>\n",
              "      <td>220.042470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>69.881796</td>\n",
              "      <td>206.349801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Male</td>\n",
              "      <td>67.253016</td>\n",
              "      <td>152.212156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Male</td>\n",
              "      <td>68.785081</td>\n",
              "      <td>183.927889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Male</td>\n",
              "      <td>68.348516</td>\n",
              "      <td>167.971111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Male</td>\n",
              "      <td>67.018950</td>\n",
              "      <td>175.929440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Male</td>\n",
              "      <td>63.456494</td>\n",
              "      <td>156.399676</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Gender     Height      Weight\n",
              "0   Male  73.847017  241.893563\n",
              "1   Male  68.781904  162.310473\n",
              "2   Male  74.110105  212.740856\n",
              "3   Male  71.730978  220.042470\n",
              "4   Male  69.881796  206.349801\n",
              "5   Male  67.253016  152.212156\n",
              "6   Male  68.785081  183.927889\n",
              "7   Male  68.348516  167.971111\n",
              "8   Male  67.018950  175.929440\n",
              "9   Male  63.456494  156.399676"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHaGOfA309JK"
      },
      "source": [
        "Convert Text Data into Dummy Variables for Machine Learning. It is important since Machine Learning models will not accept string variables directly during training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdDQXw_B0o0o"
      },
      "source": [
        "dummy_dataset = pd.DataFrame()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywob9vC33AxT",
        "outputId": "5a161f29-a660-4187-b97b-e8bb5a6a9b93"
      },
      "source": [
        "for i in range(0,len(dataset.columns)):\n",
        "  X = dataset[dataset.columns[i]]\n",
        "  if type(X[0]) == str:\n",
        "    Y = pd.get_dummies(X)\n",
        "    dummy_dataset = pd.concat([dummy_dataset, Y], axis=1)\n",
        "  else:\n",
        "    dummy_dataset = pd.concat([dummy_dataset, X], axis=1)\n",
        "#\n",
        "dummy_dataset.shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fs5TFBCvdgq"
      },
      "source": [
        "dummy_dataset['Height'] = dummy_dataset['Height']/100\n",
        "dummy_dataset['Weight'] = dummy_dataset['Weight']/100"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krHl066brWFY"
      },
      "source": [
        "Now, Male and Female each have their separate columns (i.e., each are dummy variables). It is similar to One-Hot-Encoding done for Labels for Classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "EvpjxJGc3N2B",
        "outputId": "b85b1db3-9a9a-4061-dbbd-81a3b043d22c"
      },
      "source": [
        "dummy_dataset.head(10)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Female</th>\n",
              "      <th>Male</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>73.847017</td>\n",
              "      <td>241.893563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>68.781904</td>\n",
              "      <td>162.310473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>74.110105</td>\n",
              "      <td>212.740856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>71.730978</td>\n",
              "      <td>220.042470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>69.881796</td>\n",
              "      <td>206.349801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>67.253016</td>\n",
              "      <td>152.212156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>68.785081</td>\n",
              "      <td>183.927889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>68.348516</td>\n",
              "      <td>167.971111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>67.018950</td>\n",
              "      <td>175.929440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>63.456494</td>\n",
              "      <td>156.399676</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Female  Male     Height      Weight\n",
              "0       0     1  73.847017  241.893563\n",
              "1       0     1  68.781904  162.310473\n",
              "2       0     1  74.110105  212.740856\n",
              "3       0     1  71.730978  220.042470\n",
              "4       0     1  69.881796  206.349801\n",
              "5       0     1  67.253016  152.212156\n",
              "6       0     1  68.785081  183.927889\n",
              "7       0     1  68.348516  167.971111\n",
              "8       0     1  67.018950  175.929440\n",
              "9       0     1  63.456494  156.399676"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2mdc_Xp1Jjv"
      },
      "source": [
        "Convert Pandas DataFrame into NumPy Array using 'iloc'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JGxvqvqofCA",
        "outputId": "6361511a-7ac7-438d-b1ab-720ed7b18283"
      },
      "source": [
        "X_Data = dummy_dataset.iloc[:,0:-1].values # All columns except the last are the predicting variables\n",
        "Y_Data = dummy_dataset.iloc[:,-1].values # Last column (Weight Column in this case) is the label\n",
        "print(X_Data.shape)\n",
        "print(Y_Data.shape)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIv1-vOD1W1g"
      },
      "source": [
        "Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msME9r1ookXM"
      },
      "source": [
        "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X_Data, Y_Data, test_size=0.20, random_state=42)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3WpMZUqxyGJ"
      },
      "source": [
        "The third axis is to show the number of channels, which is 1 in this case. If same labels were appropriate for more than 1 dataset, there would be more than 1 channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scbhGz6Q0Hwj"
      },
      "source": [
        "X_Train = np.expand_dims(X_Train, axis=2)\n",
        "X_Test = np.expand_dims(X_Test, axis=2)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FclyFrVloxOf",
        "outputId": "216a7759-ba77-416b-f152-d5ab3a45806d"
      },
      "source": [
        "print(X_Train.shape, X_Test.shape)\n",
        "print(Y_Train.shape, Y_Test.shape)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 3, 1) (2000, 3, 1)\n",
            "(8000,) (2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PHpFKkVyj0I"
      },
      "source": [
        "### Build and Train Imported Data using the ResNet based Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6IOf_sFx5Dm"
      },
      "source": [
        "Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFqNbe_vch4u"
      },
      "source": [
        "\"Configurations for ResNet in Regression Mode\"\n",
        "length = X_Train.shape[1]   # Number of Features (or length of the signal)\n",
        "model_width = 128           # Number of Filter or Kernel in the Input Layer\n",
        "num_channel = 1             # Number of Input Channels\n",
        "problem_type = 'Regression' # Regression or Classification\n",
        "output_number = 1           # Number of Outputs in the Regression Mode"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L543Qc_x7AB"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq-4BfWjcSHf"
      },
      "source": [
        "Regression_Model = ResNet(length, num_channel, model_width, problem_type=problem_type, output_nums=output_number).ResNet152() # Build Model\n",
        "# ResNet Models supported: ResNet18, ResNet34, ResNet50, ResNet101, ResNet152, \n",
        "Regression_Model.compile(loss='mae', optimizer='adam', metrics= ['mse']) # Compile Model\n",
        "# Here, Model validation metric is set as Mean Squared Error or MSE"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu7h2qmWx-Jg"
      },
      "source": [
        "Model_Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLz469jDhJWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd29601-0e34-4d21-a9c2-156a7d8bbac0"
      },
      "source": [
        "Regression_Model.summary() # Summary of the Model"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3, 1)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_792 (Conv1D)             (None, 2, 128)       1024        input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 1, 128)       0           conv1d_792[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_802 (Conv1D)             (None, 1, 128)       16512       max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_797 (BatchN (None, 1, 128)       512         conv1d_802[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_796 (Activation)     (None, 1, 128)       0           batch_normalization_797[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_803 (Conv1D)             (None, 1, 128)       49280       activation_796[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_798 (BatchN (None, 1, 128)       512         conv1d_803[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_797 (Activation)     (None, 1, 128)       0           batch_normalization_798[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_804 (Conv1D)             (None, 1, 512)       66048       activation_797[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_799 (BatchN (None, 1, 512)       2048        conv1d_804[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_801 (Conv1D)             (None, 1, 512)       66048       max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_798 (Activation)     (None, 1, 512)       0           batch_normalization_799[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_796 (BatchN (None, 1, 512)       2048        conv1d_801[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_190 (Add)                   (None, 1, 512)       0           activation_798[0][0]             \n",
            "                                                                 batch_normalization_796[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_799 (Activation)     (None, 1, 512)       0           add_190[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_805 (Conv1D)             (None, 1, 256)       393472      activation_799[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_800 (BatchN (None, 1, 256)       1024        conv1d_805[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_800 (Activation)     (None, 1, 256)       0           batch_normalization_800[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_806 (Conv1D)             (None, 1, 256)       196864      activation_800[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_801 (BatchN (None, 1, 256)       1024        conv1d_806[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_801 (Activation)     (None, 1, 256)       0           batch_normalization_801[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_807 (Conv1D)             (None, 1, 256)       196864      activation_801[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_802 (BatchN (None, 1, 256)       1024        conv1d_807[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_802 (Activation)     (None, 1, 256)       0           batch_normalization_802[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_833 (Conv1D)             (None, 1, 256)       65792       activation_802[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_828 (BatchN (None, 1, 256)       1024        conv1d_833[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_827 (Activation)     (None, 1, 256)       0           batch_normalization_828[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_834 (Conv1D)             (None, 1, 256)       196864      activation_827[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_829 (BatchN (None, 1, 256)       1024        conv1d_834[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_828 (Activation)     (None, 1, 256)       0           batch_normalization_829[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_835 (Conv1D)             (None, 1, 1024)      263168      activation_828[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_830 (BatchN (None, 1, 1024)      4096        conv1d_835[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_832 (Conv1D)             (None, 1, 1024)      263168      activation_802[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_829 (Activation)     (None, 1, 1024)      0           batch_normalization_830[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_827 (BatchN (None, 1, 1024)      4096        conv1d_832[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_197 (Add)                   (None, 1, 1024)      0           activation_829[0][0]             \n",
            "                                                                 batch_normalization_827[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_830 (Activation)     (None, 1, 1024)      0           add_197[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_836 (Conv1D)             (None, 1, 512)       1573376     activation_830[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_831 (BatchN (None, 1, 512)       2048        conv1d_836[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_831 (Activation)     (None, 1, 512)       0           batch_normalization_831[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_837 (Conv1D)             (None, 1, 512)       786944      activation_831[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_832 (BatchN (None, 1, 512)       2048        conv1d_837[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_832 (Activation)     (None, 1, 512)       0           batch_normalization_832[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_838 (Conv1D)             (None, 1, 512)       786944      activation_832[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_833 (BatchN (None, 1, 512)       2048        conv1d_838[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_833 (Activation)     (None, 1, 512)       0           batch_normalization_833[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_976 (Conv1D)             (None, 1, 512)       262656      activation_833[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_971 (BatchN (None, 1, 512)       2048        conv1d_976[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_970 (Activation)     (None, 1, 512)       0           batch_normalization_971[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_977 (Conv1D)             (None, 1, 512)       786944      activation_970[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_972 (BatchN (None, 1, 512)       2048        conv1d_977[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_971 (Activation)     (None, 1, 512)       0           batch_normalization_972[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_978 (Conv1D)             (None, 1, 2048)      1050624     activation_971[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_973 (BatchN (None, 1, 2048)      8192        conv1d_978[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_975 (Conv1D)             (None, 1, 2048)      1050624     activation_833[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_972 (Activation)     (None, 1, 2048)      0           batch_normalization_973[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_970 (BatchN (None, 1, 2048)      8192        conv1d_975[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_232 (Add)                   (None, 1, 2048)      0           activation_972[0][0]             \n",
            "                                                                 batch_normalization_970[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_973 (Activation)     (None, 1, 2048)      0           add_232[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_979 (Conv1D)             (None, 1, 1024)      6292480     activation_973[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_974 (BatchN (None, 1, 1024)      4096        conv1d_979[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_974 (Activation)     (None, 1, 1024)      0           batch_normalization_974[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_980 (Conv1D)             (None, 1, 1024)      3146752     activation_974[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_975 (BatchN (None, 1, 1024)      4096        conv1d_980[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_975 (Activation)     (None, 1, 1024)      0           batch_normalization_975[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_981 (Conv1D)             (None, 1, 1024)      3146752     activation_975[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_976 (BatchN (None, 1, 1024)      4096        conv1d_981[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_976 (Activation)     (None, 1, 1024)      0           batch_normalization_976[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_987 (Conv1D)             (None, 1, 1024)      1049600     activation_976[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_982 (BatchN (None, 1, 1024)      4096        conv1d_987[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_981 (Activation)     (None, 1, 1024)      0           batch_normalization_982[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_988 (Conv1D)             (None, 1, 1024)      3146752     activation_981[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_983 (BatchN (None, 1, 1024)      4096        conv1d_988[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_982 (Activation)     (None, 1, 1024)      0           batch_normalization_983[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_989 (Conv1D)             (None, 1, 4096)      4198400     activation_982[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_984 (BatchN (None, 1, 4096)      16384       conv1d_989[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_986 (Conv1D)             (None, 1, 4096)      4198400     activation_976[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_983 (Activation)     (None, 1, 4096)      0           batch_normalization_984[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_981 (BatchN (None, 1, 4096)      16384       conv1d_986[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_234 (Add)                   (None, 1, 4096)      0           activation_983[0][0]             \n",
            "                                                                 batch_normalization_981[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_984 (Activation)     (None, 1, 4096)      0           add_234[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_4 (Glo (None, 4096)         0           activation_984[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            4097        global_average_pooling1d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 33,354,753\n",
            "Trainable params: 33,305,601\n",
            "Non-trainable params: 49,152\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vHdlpJFx_14"
      },
      "source": [
        "Upload Past Weights if available (Transfer Learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSW2BfUMqs7A"
      },
      "source": [
        "Regression_Model.load_weights('Saved_Model.h5') # Load Previously Trained Weights for Transfer Learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSicHIFzyCky"
      },
      "source": [
        "Train Model for 'n' number of Epochs with Batch size of 'm'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuaVIjBviw7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af44273c-1ee1-4171-f3d4-963fbf58f2fb"
      },
      "source": [
        "# Early Stopping and Model_Checkpoints are optional parameters\n",
        "# Early Stopping is to stop the training based on certain condition set by the user\n",
        "# Model Checkpoint is to save a model in a directory based on certain conditions so that it can be used later for Transfer Learning or avoiding retraining\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=30, mode='min'), ModelCheckpoint('Saved_Model.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
        "history = Regression_Model.fit(X_Train, Y_Train, epochs=500, batch_size=128, verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks)\n",
        "# Save 'History' of the model for model performance analysis performed later"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "50/50 [==============================] - 12s 79ms/step - loss: 110.3953 - mse: 14881.6306 - val_loss: 4003.0815 - val_mse: 16147564.0000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4003.08154, saving model to Saved_Model.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 15.5558 - mse: 395.2615 - val_loss: 4144.7173 - val_mse: 17404792.0000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 4003.08154\n",
            "Epoch 3/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 12.4203 - mse: 245.3937 - val_loss: 563.2961 - val_mse: 331952.2812\n",
            "\n",
            "Epoch 00003: val_loss improved from 4003.08154 to 563.29614, saving model to Saved_Model.h5\n",
            "Epoch 4/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 9.8877 - mse: 153.1769 - val_loss: 1423.7466 - val_mse: 2236476.0000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 563.29614\n",
            "Epoch 5/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 10.5106 - mse: 172.2290 - val_loss: 43.6442 - val_mse: 2092.8853\n",
            "\n",
            "Epoch 00005: val_loss improved from 563.29614 to 43.64417, saving model to Saved_Model.h5\n",
            "Epoch 6/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.9200 - mse: 156.2534 - val_loss: 89.1450 - val_mse: 12077.4570\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 43.64417\n",
            "Epoch 7/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.3980 - mse: 138.4125 - val_loss: 30.6423 - val_mse: 1127.5370\n",
            "\n",
            "Epoch 00007: val_loss improved from 43.64417 to 30.64231, saving model to Saved_Model.h5\n",
            "Epoch 8/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.1367 - mse: 130.4771 - val_loss: 30.5724 - val_mse: 1488.4055\n",
            "\n",
            "Epoch 00008: val_loss improved from 30.64231 to 30.57239, saving model to Saved_Model.h5\n",
            "Epoch 9/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.4434 - mse: 136.8565 - val_loss: 16.4687 - val_mse: 439.8423\n",
            "\n",
            "Epoch 00009: val_loss improved from 30.57239 to 16.46869, saving model to Saved_Model.h5\n",
            "Epoch 10/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.6115 - mse: 143.7015 - val_loss: 17.7716 - val_mse: 442.6700\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 16.46869\n",
            "Epoch 11/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.0937 - mse: 130.5617 - val_loss: 12.7631 - val_mse: 240.2054\n",
            "\n",
            "Epoch 00011: val_loss improved from 16.46869 to 12.76314, saving model to Saved_Model.h5\n",
            "Epoch 12/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6395 - mse: 117.9460 - val_loss: 39.9738 - val_mse: 1921.3185\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 12.76314\n",
            "Epoch 13/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.9065 - mse: 125.3362 - val_loss: 25.6475 - val_mse: 821.4581\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 12.76314\n",
            "Epoch 14/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.5484 - mse: 142.4569 - val_loss: 50.5170 - val_mse: 2737.3037\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 12.76314\n",
            "Epoch 15/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 9.2396 - mse: 135.4461 - val_loss: 18.9077 - val_mse: 582.0210\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 12.76314\n",
            "Epoch 16/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.9412 - mse: 124.9931 - val_loss: 47.1281 - val_mse: 3222.4629\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 12.76314\n",
            "Epoch 17/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.3687 - mse: 139.0979 - val_loss: 42.4915 - val_mse: 2232.4763\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 12.76314\n",
            "Epoch 18/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.0063 - mse: 128.2709 - val_loss: 124.7217 - val_mse: 18153.7832\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 12.76314\n",
            "Epoch 19/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.0343 - mse: 127.8476 - val_loss: 31.2093 - val_mse: 1148.3975\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 12.76314\n",
            "Epoch 20/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.1604 - mse: 131.5121 - val_loss: 14.2235 - val_mse: 319.8250\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 12.76314\n",
            "Epoch 21/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8390 - mse: 122.2580 - val_loss: 47.7283 - val_mse: 2444.9326\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 12.76314\n",
            "Epoch 22/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.9178 - mse: 123.9769 - val_loss: 14.3522 - val_mse: 307.6021\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 12.76314\n",
            "Epoch 23/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.1837 - mse: 132.5094 - val_loss: 24.2618 - val_mse: 862.1811\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 12.76314\n",
            "Epoch 24/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 9.1266 - mse: 131.9900 - val_loss: 60.6272 - val_mse: 6045.0542\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 12.76314\n",
            "Epoch 25/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.2008 - mse: 133.3198 - val_loss: 67.3545 - val_mse: 5088.7905\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 12.76314\n",
            "Epoch 26/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.8513 - mse: 124.5547 - val_loss: 22.1756 - val_mse: 688.8566\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 12.76314\n",
            "Epoch 27/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.8947 - mse: 123.8699 - val_loss: 17.1699 - val_mse: 417.9758\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 12.76314\n",
            "Epoch 28/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.9878 - mse: 126.6696 - val_loss: 21.4413 - val_mse: 632.7100\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 12.76314\n",
            "Epoch 29/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.1794 - mse: 132.3386 - val_loss: 16.3577 - val_mse: 376.3375\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 12.76314\n",
            "Epoch 30/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.3644 - mse: 139.6697 - val_loss: 25.4169 - val_mse: 898.0793\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 12.76314\n",
            "Epoch 31/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.2060 - mse: 135.3753 - val_loss: 23.8585 - val_mse: 773.2769\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 12.76314\n",
            "Epoch 32/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.9249 - mse: 126.9004 - val_loss: 19.6397 - val_mse: 561.9136\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 12.76314\n",
            "Epoch 33/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8660 - mse: 122.2751 - val_loss: 12.7005 - val_mse: 249.2857\n",
            "\n",
            "Epoch 00033: val_loss improved from 12.76314 to 12.70048, saving model to Saved_Model.h5\n",
            "Epoch 34/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7169 - mse: 120.2335 - val_loss: 12.4943 - val_mse: 236.5475\n",
            "\n",
            "Epoch 00034: val_loss improved from 12.70048 to 12.49432, saving model to Saved_Model.h5\n",
            "Epoch 35/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.1979 - mse: 130.8455 - val_loss: 11.3581 - val_mse: 196.0762\n",
            "\n",
            "Epoch 00035: val_loss improved from 12.49432 to 11.35809, saving model to Saved_Model.h5\n",
            "Epoch 36/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.2810 - mse: 135.4244 - val_loss: 21.6351 - val_mse: 586.5234\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 11.35809\n",
            "Epoch 37/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.9152 - mse: 122.9835 - val_loss: 15.5816 - val_mse: 389.9043\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 11.35809\n",
            "Epoch 38/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5382 - mse: 114.2229 - val_loss: 13.5500 - val_mse: 302.8094\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 11.35809\n",
            "Epoch 39/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.8391 - mse: 122.0280 - val_loss: 11.0249 - val_mse: 195.1633\n",
            "\n",
            "Epoch 00039: val_loss improved from 11.35809 to 11.02491, saving model to Saved_Model.h5\n",
            "Epoch 40/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6107 - mse: 116.3644 - val_loss: 12.0559 - val_mse: 232.4766\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 11.02491\n",
            "Epoch 41/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.0427 - mse: 128.4927 - val_loss: 14.1876 - val_mse: 322.4719\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 11.02491\n",
            "Epoch 42/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6681 - mse: 118.7280 - val_loss: 63.2624 - val_mse: 5030.9487\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 11.02491\n",
            "Epoch 43/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5858 - mse: 116.0788 - val_loss: 36.6634 - val_mse: 1661.3949\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 11.02491\n",
            "Epoch 44/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6318 - mse: 118.3468 - val_loss: 55.4685 - val_mse: 3577.5828\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 11.02491\n",
            "Epoch 45/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7143 - mse: 118.7220 - val_loss: 50.4564 - val_mse: 3605.7168\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 11.02491\n",
            "Epoch 46/500\n",
            "50/50 [==============================] - 2s 43ms/step - loss: 9.0648 - mse: 128.8758 - val_loss: 32.2951 - val_mse: 1557.4117\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 11.02491\n",
            "Epoch 47/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.6659 - mse: 118.3420 - val_loss: 14.8182 - val_mse: 310.1899\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 11.02491\n",
            "Epoch 48/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5735 - mse: 115.4425 - val_loss: 20.1226 - val_mse: 573.7189\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 11.02491\n",
            "Epoch 49/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.7267 - mse: 119.2330 - val_loss: 28.5777 - val_mse: 1018.3948\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 11.02491\n",
            "Epoch 50/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.7411 - mse: 118.1323 - val_loss: 20.8312 - val_mse: 608.3281\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 11.02491\n",
            "Epoch 51/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.7326 - mse: 117.8502 - val_loss: 26.0565 - val_mse: 1012.8021\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 11.02491\n",
            "Epoch 52/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.4329 - mse: 140.3065 - val_loss: 47.6332 - val_mse: 3097.7566\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 11.02491\n",
            "Epoch 53/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 9.0579 - mse: 128.0446 - val_loss: 38.1144 - val_mse: 2201.6982\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 11.02491\n",
            "Epoch 54/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5912 - mse: 116.8538 - val_loss: 42.3757 - val_mse: 2110.3914\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 11.02491\n",
            "Epoch 55/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6252 - mse: 116.6469 - val_loss: 17.8526 - val_mse: 453.0434\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 11.02491\n",
            "Epoch 56/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8361 - mse: 123.1364 - val_loss: 14.0621 - val_mse: 307.8436\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 11.02491\n",
            "Epoch 57/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6325 - mse: 115.9319 - val_loss: 11.7502 - val_mse: 217.8046\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 11.02491\n",
            "Epoch 58/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8535 - mse: 121.6836 - val_loss: 31.1500 - val_mse: 1288.9762\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 11.02491\n",
            "Epoch 59/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8795 - mse: 123.2678 - val_loss: 15.7530 - val_mse: 363.9751\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 11.02491\n",
            "Epoch 60/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.6009 - mse: 115.7707 - val_loss: 12.8149 - val_mse: 250.9737\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 11.02491\n",
            "Epoch 61/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7019 - mse: 117.8526 - val_loss: 20.8105 - val_mse: 583.8539\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 11.02491\n",
            "Epoch 62/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.9162 - mse: 124.6769 - val_loss: 49.0407 - val_mse: 3555.8018\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 11.02491\n",
            "Epoch 63/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.9148 - mse: 122.8824 - val_loss: 14.7273 - val_mse: 323.3656\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 11.02491\n",
            "Epoch 64/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8578 - mse: 121.9418 - val_loss: 27.1218 - val_mse: 1156.5856\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 11.02491\n",
            "Epoch 65/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8368 - mse: 120.6873 - val_loss: 20.9317 - val_mse: 672.8109\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 11.02491\n",
            "Epoch 66/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8284 - mse: 121.3909 - val_loss: 25.9797 - val_mse: 1060.0454\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 11.02491\n",
            "Epoch 67/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7421 - mse: 119.7477 - val_loss: 11.9557 - val_mse: 213.0596\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 11.02491\n",
            "Epoch 68/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6565 - mse: 114.9784 - val_loss: 10.9365 - val_mse: 181.8170\n",
            "\n",
            "Epoch 00068: val_loss improved from 11.02491 to 10.93653, saving model to Saved_Model.h5\n",
            "Epoch 69/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.5228 - mse: 113.5230 - val_loss: 9.6055 - val_mse: 145.0042\n",
            "\n",
            "Epoch 00069: val_loss improved from 10.93653 to 9.60546, saving model to Saved_Model.h5\n",
            "Epoch 70/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.6059 - mse: 114.6025 - val_loss: 9.5920 - val_mse: 145.1594\n",
            "\n",
            "Epoch 00070: val_loss improved from 9.60546 to 9.59203, saving model to Saved_Model.h5\n",
            "Epoch 71/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 9.0156 - mse: 125.9993 - val_loss: 10.3257 - val_mse: 166.4441\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 9.59203\n",
            "Epoch 72/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7440 - mse: 121.3698 - val_loss: 9.7015 - val_mse: 150.2346\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 9.59203\n",
            "Epoch 73/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6568 - mse: 119.4964 - val_loss: 9.1090 - val_mse: 131.6756\n",
            "\n",
            "Epoch 00073: val_loss improved from 9.59203 to 9.10897, saving model to Saved_Model.h5\n",
            "Epoch 74/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 9.0117 - mse: 126.8714 - val_loss: 19.3867 - val_mse: 532.3735\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 9.10897\n",
            "Epoch 75/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.8103 - mse: 121.7050 - val_loss: 15.1432 - val_mse: 374.7405\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 9.10897\n",
            "Epoch 76/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6890 - mse: 117.6937 - val_loss: 11.2340 - val_mse: 195.3940\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 9.10897\n",
            "Epoch 77/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5744 - mse: 116.1145 - val_loss: 14.9683 - val_mse: 343.1404\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 9.10897\n",
            "Epoch 78/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8835 - mse: 125.2004 - val_loss: 30.2926 - val_mse: 1234.0193\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 9.10897\n",
            "Epoch 79/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.9120 - mse: 124.0626 - val_loss: 26.3881 - val_mse: 862.3218\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 9.10897\n",
            "Epoch 80/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.6398 - mse: 116.9223 - val_loss: 26.2023 - val_mse: 827.5206\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 9.10897\n",
            "Epoch 81/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5910 - mse: 115.0045 - val_loss: 14.4071 - val_mse: 301.4743\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 9.10897\n",
            "Epoch 82/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.4954 - mse: 113.5253 - val_loss: 30.6419 - val_mse: 1139.9630\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 9.10897\n",
            "Epoch 83/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8248 - mse: 122.1697 - val_loss: 15.5775 - val_mse: 362.1867\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 9.10897\n",
            "Epoch 84/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5672 - mse: 115.1597 - val_loss: 8.8538 - val_mse: 125.8301\n",
            "\n",
            "Epoch 00084: val_loss improved from 9.10897 to 8.85383, saving model to Saved_Model.h5\n",
            "Epoch 85/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.9437 - mse: 124.4179 - val_loss: 13.6981 - val_mse: 282.1111\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 8.85383\n",
            "Epoch 86/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5473 - mse: 113.4981 - val_loss: 15.8119 - val_mse: 397.5670\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 8.85383\n",
            "Epoch 87/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.9612 - mse: 126.6634 - val_loss: 20.7521 - val_mse: 580.0975\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 8.85383\n",
            "Epoch 88/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6140 - mse: 115.7355 - val_loss: 19.1566 - val_mse: 549.7530\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 8.85383\n",
            "Epoch 89/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.7921 - mse: 123.1388 - val_loss: 31.0026 - val_mse: 1173.0691\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 8.85383\n",
            "Epoch 90/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.5936 - mse: 116.0924 - val_loss: 17.6580 - val_mse: 462.3852\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 8.85383\n",
            "Epoch 91/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.6220 - mse: 116.4798 - val_loss: 36.1732 - val_mse: 1607.3448\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 8.85383\n",
            "Epoch 92/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5364 - mse: 114.0082 - val_loss: 24.5197 - val_mse: 756.2617\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 8.85383\n",
            "Epoch 93/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6920 - mse: 117.7364 - val_loss: 15.6346 - val_mse: 353.6767\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 8.85383\n",
            "Epoch 94/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6637 - mse: 116.9361 - val_loss: 23.5445 - val_mse: 786.9760\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 8.85383\n",
            "Epoch 95/500\n",
            "50/50 [==============================] - 2s 43ms/step - loss: 8.7474 - mse: 120.9315 - val_loss: 14.1750 - val_mse: 335.5274\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 8.85383\n",
            "Epoch 96/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6323 - mse: 115.8990 - val_loss: 14.5837 - val_mse: 295.1885\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 8.85383\n",
            "Epoch 97/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8701 - mse: 123.0669 - val_loss: 43.8843 - val_mse: 2426.2432\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 8.85383\n",
            "Epoch 98/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.9452 - mse: 124.6807 - val_loss: 58.1742 - val_mse: 3967.1606\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 8.85383\n",
            "Epoch 99/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7997 - mse: 121.9136 - val_loss: 26.0539 - val_mse: 1008.5709\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 8.85383\n",
            "Epoch 100/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.9304 - mse: 124.2764 - val_loss: 20.0941 - val_mse: 564.8106\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 8.85383\n",
            "Epoch 101/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8138 - mse: 122.6601 - val_loss: 9.8772 - val_mse: 151.3272\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 8.85383\n",
            "Epoch 102/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.4869 - mse: 112.6313 - val_loss: 8.7601 - val_mse: 121.4872\n",
            "\n",
            "Epoch 00102: val_loss improved from 8.85383 to 8.76006, saving model to Saved_Model.h5\n",
            "Epoch 103/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.6240 - mse: 116.4305 - val_loss: 19.4286 - val_mse: 514.8486\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 8.76006\n",
            "Epoch 104/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.7562 - mse: 120.8683 - val_loss: 27.1685 - val_mse: 1063.4258\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 8.76006\n",
            "Epoch 105/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.8421 - mse: 124.1884 - val_loss: 66.7652 - val_mse: 5139.8223\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 8.76006\n",
            "Epoch 106/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.6327 - mse: 116.6703 - val_loss: 15.3880 - val_mse: 348.1938\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 8.76006\n",
            "Epoch 107/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.8474 - mse: 122.2781 - val_loss: 13.9175 - val_mse: 308.4329\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 8.76006\n",
            "Epoch 108/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6686 - mse: 117.1077 - val_loss: 10.1142 - val_mse: 157.0748\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 8.76006\n",
            "Epoch 109/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8238 - mse: 121.1404 - val_loss: 16.0329 - val_mse: 382.4709\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 8.76006\n",
            "Epoch 110/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6480 - mse: 117.4850 - val_loss: 51.5658 - val_mse: 3072.8257\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 8.76006\n",
            "Epoch 111/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7112 - mse: 118.0272 - val_loss: 47.6904 - val_mse: 2856.9407\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 8.76006\n",
            "Epoch 112/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.8297 - mse: 123.5507 - val_loss: 36.1012 - val_mse: 1514.8135\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 8.76006\n",
            "Epoch 113/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7006 - mse: 117.1717 - val_loss: 24.2677 - val_mse: 885.8473\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 8.76006\n",
            "Epoch 114/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.4593 - mse: 112.0093 - val_loss: 17.7090 - val_mse: 479.5035\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 8.76006\n",
            "Epoch 115/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.4833 - mse: 113.0099 - val_loss: 15.8539 - val_mse: 381.4063\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 8.76006\n",
            "Epoch 116/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.4581 - mse: 111.1164 - val_loss: 14.2203 - val_mse: 293.7936\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 8.76006\n",
            "Epoch 117/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.4828 - mse: 112.8011 - val_loss: 19.8887 - val_mse: 550.7246\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 8.76006\n",
            "Epoch 118/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6289 - mse: 116.4201 - val_loss: 16.4198 - val_mse: 398.5782\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 8.76006\n",
            "Epoch 119/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.4113 - mse: 110.9116 - val_loss: 24.9170 - val_mse: 797.9644\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 8.76006\n",
            "Epoch 120/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7464 - mse: 119.6731 - val_loss: 25.0746 - val_mse: 817.1548\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 8.76006\n",
            "Epoch 121/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.4608 - mse: 112.1692 - val_loss: 17.6684 - val_mse: 428.9813\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 8.76006\n",
            "Epoch 122/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6681 - mse: 114.7339 - val_loss: 16.4708 - val_mse: 414.8394\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 8.76006\n",
            "Epoch 123/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.4842 - mse: 112.1969 - val_loss: 31.1952 - val_mse: 1490.0739\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 8.76006\n",
            "Epoch 124/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.7041 - mse: 119.8461 - val_loss: 40.9896 - val_mse: 1941.8553\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 8.76006\n",
            "Epoch 125/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6146 - mse: 116.8151 - val_loss: 12.0210 - val_mse: 214.8572\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 8.76006\n",
            "Epoch 126/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.5227 - mse: 113.2523 - val_loss: 12.3706 - val_mse: 235.5157\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 8.76006\n",
            "Epoch 127/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6437 - mse: 117.2962 - val_loss: 13.4983 - val_mse: 274.5829\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 8.76006\n",
            "Epoch 128/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.6172 - mse: 115.7546 - val_loss: 29.1721 - val_mse: 986.2974\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 8.76006\n",
            "Epoch 129/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.4337 - mse: 111.8755 - val_loss: 13.2846 - val_mse: 288.1180\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 8.76006\n",
            "Epoch 130/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.9932 - mse: 124.8816 - val_loss: 44.5856 - val_mse: 2895.2100\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 8.76006\n",
            "Epoch 131/500\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 8.4882 - mse: 113.0290 - val_loss: 33.0855 - val_mse: 1724.2050\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 8.76006\n",
            "Epoch 132/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 8.6026 - mse: 116.3740 - val_loss: 24.1251 - val_mse: 814.6142\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 8.76006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw0HsDDtyEuv"
      },
      "source": [
        "Test and Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs6vtdMjxRx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095b79fa-80bb-4a4d-9009-01e78ef283c6"
      },
      "source": [
        "# Preictions from the Test Set from the Trained Model\n",
        "Predictions = Regression_Model.predict(X_Test, verbose=1)\n",
        "print(Predictions.shape)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 2s 10ms/step\n",
            "(2000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIsSHpXKyGMg"
      },
      "source": [
        "Error Performance (Mean Sqaured Error or MAE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gs9TFwlxiYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65edc7d-4f0c-4a3d-a81d-cd77f323bcf8"
      },
      "source": [
        "# Error of the prediction, one of many evaluation metrics\n",
        "# Using Mean Absolute Error (MAE) in this case as a sample\n",
        "Error = mean_absolute_error(Y_Test, Predictions)\n",
        "print(f\"MAE: {Error}\")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: 24.016691516831454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okQxrKA4yHUh"
      },
      "source": [
        "Plot Train and Validation Error and Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "Y2fQAxGrN_jo",
        "outputId": "e07a1d0d-96e4-4cb9-cbb0-40dc7a187b6a"
      },
      "source": [
        "def history_plot(history):\n",
        "  # list all dictionaries in history\n",
        "  print(history.history.keys())\n",
        "  # summarize history for error\n",
        "  plt.figure(figsize=(12,10))\n",
        "  plt.subplot(2,1,1)\n",
        "  plt.plot(history.history['mse'])\n",
        "  plt.plot(history.history['val_mse'])\n",
        "  plt.title('Model Error Performance')\n",
        "  plt.ylabel('Error')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper right')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.figure(figsize=(12,10))\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper right')\n",
        "  plt.show()\n",
        "#\n",
        "history_plot(history)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9nUwaevyK4u"
      },
      "source": [
        "Plot Prediction Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "FjvG2bmdx0D1",
        "outputId": "992ccdd0-7150-40cb-94bd-474b98990579"
      },
      "source": [
        "def plot_prediction(test_labels, test_predictions):\n",
        "  plt.figure(figsize=(12,10))\n",
        "  plt.subplot(2,1,1)\n",
        "  plt.scatter(test_labels, test_predictions)\n",
        "  plt.title('Ground Truth vs. Prediction Scatter Plot')\n",
        "  plt.xlabel('Ground Truth')\n",
        "  plt.ylabel('Predictions')\n",
        "  plt.axis('equal')\n",
        "  plt.xlim(plt.xlim())\n",
        "  plt.ylim(plt.ylim())\n",
        "  _ = plt.plot([-100, 100],[-100,100])\n",
        "  #\n",
        "  plt.figure(figsize=(12,10))\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.title('Error Histogram')\n",
        "  error = test_predictions - test_labels\n",
        "  plt.hist(error, bins = 50)\n",
        "  plt.xlabel(\"Prediction Error\")\n",
        "  _ = plt.ylabel(\"Count\")\n",
        "#\n",
        "plot_prediction(Y_Test, Predictions)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQFYFuosuebY"
      },
      "source": [
        "You can perform futher analysis of the performance for potential improvement or even compare with other models. The dataset also can be preprocessed differently. For example, the model might have performed better if the values were normalized in the same way?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTK7pWuSt_bg"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kafC_zqScLpS"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS9ECofK3Yee"
      },
      "source": [
        "Import the dataset from a CSV file and save it into a Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "NqOiFHn7oYPo",
        "outputId": "32ac68e3-359b-4a88-bddc-76457ae1c71e"
      },
      "source": [
        "dataset = pd.read_excel('Drug_Persistency.xlsx', sheet_name = 'Dataset')\n",
        "dataset.drop(columns=['Ptid'], inplace=True) # Drop Patient ID Column\n",
        "print(dataset.shape)\n",
        "dataset.head(10)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3424, 68)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Persistency_Flag</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Race</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>Region</th>\n",
              "      <th>Age_Bucket</th>\n",
              "      <th>Ntm_Speciality</th>\n",
              "      <th>Ntm_Specialist_Flag</th>\n",
              "      <th>Ntm_Speciality_Bucket</th>\n",
              "      <th>Gluco_Record_Prior_Ntm</th>\n",
              "      <th>Gluco_Record_During_Rx</th>\n",
              "      <th>Dexa_Freq_During_Rx</th>\n",
              "      <th>Dexa_During_Rx</th>\n",
              "      <th>Frag_Frac_Prior_Ntm</th>\n",
              "      <th>Frag_Frac_During_Rx</th>\n",
              "      <th>Risk_Segment_Prior_Ntm</th>\n",
              "      <th>Tscore_Bucket_Prior_Ntm</th>\n",
              "      <th>Risk_Segment_During_Rx</th>\n",
              "      <th>Tscore_Bucket_During_Rx</th>\n",
              "      <th>Change_T_Score</th>\n",
              "      <th>Change_Risk_Segment</th>\n",
              "      <th>Adherent_Flag</th>\n",
              "      <th>Idn_Indicator</th>\n",
              "      <th>Injectable_Experience_During_Rx</th>\n",
              "      <th>Comorb_Encounter_For_Screening_For_Malignant_Neoplasms</th>\n",
              "      <th>Comorb_Encounter_For_Immunization</th>\n",
              "      <th>Comorb_Encntr_For_General_Exam_W_O_Complaint,_Susp_Or_Reprtd_Dx</th>\n",
              "      <th>Comorb_Vitamin_D_Deficiency</th>\n",
              "      <th>Comorb_Other_Joint_Disorder_Not_Elsewhere_Classified</th>\n",
              "      <th>Comorb_Encntr_For_Oth_Sp_Exam_W_O_Complaint_Suspected_Or_Reprtd_Dx</th>\n",
              "      <th>Comorb_Long_Term_Current_Drug_Therapy</th>\n",
              "      <th>Comorb_Dorsalgia</th>\n",
              "      <th>Comorb_Personal_History_Of_Other_Diseases_And_Conditions</th>\n",
              "      <th>Comorb_Other_Disorders_Of_Bone_Density_And_Structure</th>\n",
              "      <th>Comorb_Disorders_of_lipoprotein_metabolism_and_other_lipidemias</th>\n",
              "      <th>Comorb_Osteoporosis_without_current_pathological_fracture</th>\n",
              "      <th>Comorb_Personal_history_of_malignant_neoplasm</th>\n",
              "      <th>Comorb_Gastro_esophageal_reflux_disease</th>\n",
              "      <th>Concom_Cholesterol_And_Triglyceride_Regulating_Preparations</th>\n",
              "      <th>Concom_Narcotics</th>\n",
              "      <th>Concom_Systemic_Corticosteroids_Plain</th>\n",
              "      <th>Concom_Anti_Depressants_And_Mood_Stabilisers</th>\n",
              "      <th>Concom_Fluoroquinolones</th>\n",
              "      <th>Concom_Cephalosporins</th>\n",
              "      <th>Concom_Macrolides_And_Similar_Types</th>\n",
              "      <th>Concom_Broad_Spectrum_Penicillins</th>\n",
              "      <th>Concom_Anaesthetics_General</th>\n",
              "      <th>Concom_Viral_Vaccines</th>\n",
              "      <th>Risk_Type_1_Insulin_Dependent_Diabetes</th>\n",
              "      <th>Risk_Osteogenesis_Imperfecta</th>\n",
              "      <th>Risk_Rheumatoid_Arthritis</th>\n",
              "      <th>Risk_Untreated_Chronic_Hyperthyroidism</th>\n",
              "      <th>Risk_Untreated_Chronic_Hypogonadism</th>\n",
              "      <th>Risk_Untreated_Early_Menopause</th>\n",
              "      <th>Risk_Patient_Parent_Fractured_Their_Hip</th>\n",
              "      <th>Risk_Smoking_Tobacco</th>\n",
              "      <th>Risk_Chronic_Malnutrition_Or_Malabsorption</th>\n",
              "      <th>Risk_Chronic_Liver_Disease</th>\n",
              "      <th>Risk_Family_History_Of_Osteoporosis</th>\n",
              "      <th>Risk_Low_Calcium_Intake</th>\n",
              "      <th>Risk_Vitamin_D_Insufficiency</th>\n",
              "      <th>Risk_Poor_Health_Frailty</th>\n",
              "      <th>Risk_Excessive_Thinness</th>\n",
              "      <th>Risk_Hysterectomy_Oophorectomy</th>\n",
              "      <th>Risk_Estrogen_Deficiency</th>\n",
              "      <th>Risk_Immobilization</th>\n",
              "      <th>Risk_Recurring_Falls</th>\n",
              "      <th>Count_Of_Risks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Persistent</td>\n",
              "      <td>Male</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Not Hispanic</td>\n",
              "      <td>West</td>\n",
              "      <td>&gt;75</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>VLR_LR</td>\n",
              "      <td>&gt;-2.5</td>\n",
              "      <td>VLR_LR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>No change</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non-Persistent</td>\n",
              "      <td>Male</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Not Hispanic</td>\n",
              "      <td>West</td>\n",
              "      <td>55-65</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>VLR_LR</td>\n",
              "      <td>&gt;-2.5</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non-Persistent</td>\n",
              "      <td>Female</td>\n",
              "      <td>Other/Unknown</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>Midwest</td>\n",
              "      <td>65-75</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>No change</td>\n",
              "      <td>No change</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non-Persistent</td>\n",
              "      <td>Female</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Not Hispanic</td>\n",
              "      <td>Midwest</td>\n",
              "      <td>&gt;75</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&gt;-2.5</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>No change</td>\n",
              "      <td>No change</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non-Persistent</td>\n",
              "      <td>Female</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Not Hispanic</td>\n",
              "      <td>Midwest</td>\n",
              "      <td>&gt;75</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Non-Persistent</td>\n",
              "      <td>Female</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Not Hispanic</td>\n",
              "      <td>Midwest</td>\n",
              "      <td>&gt;75</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Non-Persistent</td>\n",
              "      <td>Female</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Not Hispanic</td>\n",
              "      <td>Midwest</td>\n",
              "      <td>&gt;75</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>2</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Non-Persistent</td>\n",
              "      <td>Female</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Not Hispanic</td>\n",
              "      <td>Midwest</td>\n",
              "      <td>&gt;75</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>No change</td>\n",
              "      <td>No change</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Non-Persistent</td>\n",
              "      <td>Female</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Not Hispanic</td>\n",
              "      <td>Midwest</td>\n",
              "      <td>&gt;75</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>VLR_LR</td>\n",
              "      <td>&gt;-2.5</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Non-Persistent</td>\n",
              "      <td>Female</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Not Hispanic</td>\n",
              "      <td>Midwest</td>\n",
              "      <td>&gt;75</td>\n",
              "      <td>GENERAL PRACTITIONER</td>\n",
              "      <td>Others</td>\n",
              "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>HR_VHR</td>\n",
              "      <td>&lt;=-2.5</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Adherent</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Persistency_Flag  Gender  ... Risk_Recurring_Falls Count_Of_Risks\n",
              "0       Persistent    Male  ...                    N              0\n",
              "1   Non-Persistent    Male  ...                    N              0\n",
              "2   Non-Persistent  Female  ...                    N              2\n",
              "3   Non-Persistent  Female  ...                    N              1\n",
              "4   Non-Persistent  Female  ...                    N              1\n",
              "5   Non-Persistent  Female  ...                    N              2\n",
              "6   Non-Persistent  Female  ...                    N              1\n",
              "7   Non-Persistent  Female  ...                    N              1\n",
              "8   Non-Persistent  Female  ...                    N              1\n",
              "9   Non-Persistent  Female  ...                    N              1\n",
              "\n",
              "[10 rows x 68 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHcF1oUr3eYv"
      },
      "source": [
        "Create a Blank DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2WTdfGPdY2t"
      },
      "source": [
        "dummy_dataset = pd.DataFrame()"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "PWG1U1PtoB_b",
        "outputId": "d2e4065a-8fd0-4c62-9822-dce0060cf362"
      },
      "source": [
        "for i in range(0,len(dataset.columns)):\n",
        "  X = dataset[dataset.columns[i]]\n",
        "  if type(X[0]) == str:\n",
        "    Y = pd.get_dummies(X)\n",
        "    dummy_dataset = pd.concat([dummy_dataset, Y], axis=1)\n",
        "  else:\n",
        "    dummy_dataset = pd.concat([dummy_dataset, X], axis=1)\n",
        "#\n",
        "dummy_dataset.head(10)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Non-Persistent</th>\n",
              "      <th>Persistent</th>\n",
              "      <th>Female</th>\n",
              "      <th>Male</th>\n",
              "      <th>African American</th>\n",
              "      <th>Asian</th>\n",
              "      <th>Caucasian</th>\n",
              "      <th>Other/Unknown</th>\n",
              "      <th>Hispanic</th>\n",
              "      <th>Not Hispanic</th>\n",
              "      <th>Unknown</th>\n",
              "      <th>Midwest</th>\n",
              "      <th>Northeast</th>\n",
              "      <th>Other/Unknown</th>\n",
              "      <th>South</th>\n",
              "      <th>West</th>\n",
              "      <th>55-65</th>\n",
              "      <th>65-75</th>\n",
              "      <th>&lt;55</th>\n",
              "      <th>&gt;75</th>\n",
              "      <th>CARDIOLOGY</th>\n",
              "      <th>CLINICAL NURSE SPECIALIST</th>\n",
              "      <th>EMERGENCY MEDICINE</th>\n",
              "      <th>ENDOCRINOLOGY</th>\n",
              "      <th>GASTROENTEROLOGY</th>\n",
              "      <th>GENERAL PRACTITIONER</th>\n",
              "      <th>GERIATRIC MEDICINE</th>\n",
              "      <th>HEMATOLOGY &amp; ONCOLOGY</th>\n",
              "      <th>HOSPICE AND PALLIATIVE MEDICINE</th>\n",
              "      <th>HOSPITAL MEDICINE</th>\n",
              "      <th>NEPHROLOGY</th>\n",
              "      <th>NEUROLOGY</th>\n",
              "      <th>NUCLEAR MEDICINE</th>\n",
              "      <th>OBSTETRICS &amp; OBSTETRICS &amp; GYNECOLOGY &amp; OBSTETRICS &amp; GYNECOLOGY</th>\n",
              "      <th>OBSTETRICS AND GYNECOLOGY</th>\n",
              "      <th>OCCUPATIONAL MEDICINE</th>\n",
              "      <th>ONCOLOGY</th>\n",
              "      <th>OPHTHALMOLOGY</th>\n",
              "      <th>ORTHOPEDIC SURGERY</th>\n",
              "      <th>ORTHOPEDICS</th>\n",
              "      <th>...</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>N</th>\n",
              "      <th>Y</th>\n",
              "      <th>Count_Of_Risks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows  183 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Non-Persistent  Persistent  Female  Male  ...  Y  N  Y  Count_Of_Risks\n",
              "0               0           1       0     1  ...  0  1  0               0\n",
              "1               1           0       0     1  ...  0  1  0               0\n",
              "2               1           0       1     0  ...  0  1  0               2\n",
              "3               1           0       1     0  ...  0  1  0               1\n",
              "4               1           0       1     0  ...  0  1  0               1\n",
              "5               1           0       1     0  ...  0  1  0               2\n",
              "6               1           0       1     0  ...  0  1  0               1\n",
              "7               1           0       1     0  ...  0  1  0               1\n",
              "8               1           0       1     0  ...  0  1  0               1\n",
              "9               1           0       1     0  ...  0  1  0               1\n",
              "\n",
              "[10 rows x 183 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bYbyjLA3QZg"
      },
      "source": [
        "Convert the DataFrame to a Numpy Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEE7BjUickCW",
        "outputId": "dc5d0029-bfbf-47c3-fe55-64189468ec17"
      },
      "source": [
        "X_Data = dummy_dataset.iloc[:,0:-1].values\n",
        "Y_Data = dummy_dataset.iloc[:,-1].values\n",
        "print(X_Data.shape)\n",
        "print(Y_Data.shape)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3424, 182)\n",
            "(3424,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pHZyRVJckFj"
      },
      "source": [
        "X_Train, X_Test, Y_Train_, Y_Test_ = train_test_split(X_Data, Y_Data, test_size=0.20, random_state=42)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14lorCF1QeKt"
      },
      "source": [
        "One-Hot-Encoding for the Classification Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNRWfSr32JJz"
      },
      "source": [
        "def one_hot_encoding(data):\n",
        "  integer_encoded = LabelEncoder().fit_transform(data)\n",
        "  onehot_encoder = OneHotEncoder(sparse=False)\n",
        "  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "  one_hot_encoded_data = onehot_encoder.fit_transform(integer_encoded)\n",
        "  return one_hot_encoded_data"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyG5TFrw2oSw"
      },
      "source": [
        "Y_Train = one_hot_encoding(Y_Train_)\n",
        "Y_Test = one_hot_encoding(Y_Test_)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEspqzt13M_e"
      },
      "source": [
        "Train and Test Data Shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjKHjqngcyHg",
        "outputId": "41f93d7b-f147-4363-a317-1dad9a2a8dc0"
      },
      "source": [
        "print(X_Train.shape, X_Test.shape)\n",
        "print(Y_Train.shape, Y_Test.shape)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2739, 182) (685, 182)\n",
            "(2739, 8) (685, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrzoMu4eqKU2"
      },
      "source": [
        "### Build and Train Imported Data using the ResNet based Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3HXCkvxuXVs"
      },
      "source": [
        "Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2-0A7qtuAqh"
      },
      "source": [
        "\"Configurations for ResNet in Classification Mode\"\n",
        "length = X_Train.shape[1]       # Number of Features (or length of the signal)\n",
        "model_width = 32                # Number of Input Channels\n",
        "problem_type = 'Classification' # Regression or Classification\n",
        "class_number = Y_Train.shape[1] # Number of Output Class in Classification Mode (>=2)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avEDbEMFv__K"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOWNVoQhujq7"
      },
      "source": [
        "Classification_Model = ResNet(length, num_channel, model_width, problem_type=problem_type, output_nums=class_number).ResNet152() # Change the ResNet Model if needed\n",
        "if class_number == 2:\n",
        "  Classification_Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mse'])\n",
        "elif class_number > 2:\n",
        "  Classification_Model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['mse'])"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFfydicGwAXQ"
      },
      "source": [
        "Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmdOJHbbv_nc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136dacae-b216-4725-b2be-6c777b8974c9"
      },
      "source": [
        "Classification_Model.summary()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 182, 1)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1188 (Conv1D)            (None, 91, 32)       256         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 45, 32)       0           conv1d_1188[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1198 (Conv1D)            (None, 45, 32)       1056        max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1191 (Batch (None, 45, 32)       128         conv1d_1198[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1190 (Activation)    (None, 45, 32)       0           batch_normalization_1191[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1199 (Conv1D)            (None, 45, 32)       3104        activation_1190[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1192 (Batch (None, 45, 32)       128         conv1d_1199[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1191 (Activation)    (None, 45, 32)       0           batch_normalization_1192[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1200 (Conv1D)            (None, 45, 128)      4224        activation_1191[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1193 (Batch (None, 45, 128)      512         conv1d_1200[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1197 (Conv1D)            (None, 45, 128)      4224        max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1192 (Activation)    (None, 45, 128)      0           batch_normalization_1193[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1190 (Batch (None, 45, 128)      512         conv1d_1197[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_284 (Add)                   (None, 45, 128)      0           activation_1192[0][0]            \n",
            "                                                                 batch_normalization_1190[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1193 (Activation)    (None, 45, 128)      0           add_284[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1201 (Conv1D)            (None, 23, 64)       24640       activation_1193[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1194 (Batch (None, 23, 64)       256         conv1d_1201[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1194 (Activation)    (None, 23, 64)       0           batch_normalization_1194[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1202 (Conv1D)            (None, 12, 64)       12352       activation_1194[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1195 (Batch (None, 12, 64)       256         conv1d_1202[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1195 (Activation)    (None, 12, 64)       0           batch_normalization_1195[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1203 (Conv1D)            (None, 6, 64)        12352       activation_1195[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1196 (Batch (None, 6, 64)        256         conv1d_1203[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1196 (Activation)    (None, 6, 64)        0           batch_normalization_1196[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1229 (Conv1D)            (None, 6, 64)        4160        activation_1196[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1222 (Batch (None, 6, 64)        256         conv1d_1229[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1221 (Activation)    (None, 6, 64)        0           batch_normalization_1222[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1230 (Conv1D)            (None, 6, 64)        12352       activation_1221[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1223 (Batch (None, 6, 64)        256         conv1d_1230[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1222 (Activation)    (None, 6, 64)        0           batch_normalization_1223[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1231 (Conv1D)            (None, 6, 256)       16640       activation_1222[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1224 (Batch (None, 6, 256)       1024        conv1d_1231[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1228 (Conv1D)            (None, 6, 256)       16640       activation_1196[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1223 (Activation)    (None, 6, 256)       0           batch_normalization_1224[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1221 (Batch (None, 6, 256)       1024        conv1d_1228[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_291 (Add)                   (None, 6, 256)       0           activation_1223[0][0]            \n",
            "                                                                 batch_normalization_1221[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1224 (Activation)    (None, 6, 256)       0           add_291[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1232 (Conv1D)            (None, 3, 128)       98432       activation_1224[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1225 (Batch (None, 3, 128)       512         conv1d_1232[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1225 (Activation)    (None, 3, 128)       0           batch_normalization_1225[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1233 (Conv1D)            (None, 2, 128)       49280       activation_1225[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1226 (Batch (None, 2, 128)       512         conv1d_1233[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1226 (Activation)    (None, 2, 128)       0           batch_normalization_1226[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1234 (Conv1D)            (None, 1, 128)       49280       activation_1226[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1227 (Batch (None, 1, 128)       512         conv1d_1234[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1227 (Activation)    (None, 1, 128)       0           batch_normalization_1227[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1372 (Conv1D)            (None, 1, 128)       16512       activation_1227[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1365 (Batch (None, 1, 128)       512         conv1d_1372[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1364 (Activation)    (None, 1, 128)       0           batch_normalization_1365[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1373 (Conv1D)            (None, 1, 128)       49280       activation_1364[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1366 (Batch (None, 1, 128)       512         conv1d_1373[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1365 (Activation)    (None, 1, 128)       0           batch_normalization_1366[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1374 (Conv1D)            (None, 1, 512)       66048       activation_1365[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1367 (Batch (None, 1, 512)       2048        conv1d_1374[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1371 (Conv1D)            (None, 1, 512)       66048       activation_1227[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1366 (Activation)    (None, 1, 512)       0           batch_normalization_1367[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1364 (Batch (None, 1, 512)       2048        conv1d_1371[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_326 (Add)                   (None, 1, 512)       0           activation_1366[0][0]            \n",
            "                                                                 batch_normalization_1364[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1367 (Activation)    (None, 1, 512)       0           add_326[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1375 (Conv1D)            (None, 1, 256)       393472      activation_1367[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1368 (Batch (None, 1, 256)       1024        conv1d_1375[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1368 (Activation)    (None, 1, 256)       0           batch_normalization_1368[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1376 (Conv1D)            (None, 1, 256)       196864      activation_1368[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1369 (Batch (None, 1, 256)       1024        conv1d_1376[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1369 (Activation)    (None, 1, 256)       0           batch_normalization_1369[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1377 (Conv1D)            (None, 1, 256)       196864      activation_1369[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1370 (Batch (None, 1, 256)       1024        conv1d_1377[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1370 (Activation)    (None, 1, 256)       0           batch_normalization_1370[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1383 (Conv1D)            (None, 1, 256)       65792       activation_1370[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1376 (Batch (None, 1, 256)       1024        conv1d_1383[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1375 (Activation)    (None, 1, 256)       0           batch_normalization_1376[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1384 (Conv1D)            (None, 1, 256)       196864      activation_1375[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1377 (Batch (None, 1, 256)       1024        conv1d_1384[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1376 (Activation)    (None, 1, 256)       0           batch_normalization_1377[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1385 (Conv1D)            (None, 1, 1024)      263168      activation_1376[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1378 (Batch (None, 1, 1024)      4096        conv1d_1385[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1382 (Conv1D)            (None, 1, 1024)      263168      activation_1370[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1377 (Activation)    (None, 1, 1024)      0           batch_normalization_1378[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1375 (Batch (None, 1, 1024)      4096        conv1d_1382[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_328 (Add)                   (None, 1, 1024)      0           activation_1377[0][0]            \n",
            "                                                                 batch_normalization_1375[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1378 (Activation)    (None, 1, 1024)      0           add_328[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_6 (Glo (None, 1024)         0           activation_1378[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 8)            8200        global_average_pooling1d_6[0][0] \n",
            "==================================================================================================\n",
            "Total params: 2,115,848\n",
            "Trainable params: 2,103,560\n",
            "Non-trainable params: 12,288\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HD9FVnew55K"
      },
      "source": [
        "Upload Past Weights (Transfer Learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wcSeUqAqQS0"
      },
      "source": [
        "Classification_Model.load_weights('Saved_Classification_Model.h5') # Load Previously Trained Weights for Transfer Learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8m3x9AOqQc8"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wq615uRw59y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543407a6-20b9-42a6-d7c8-4ebacc42d28b"
      },
      "source": [
        "# Early Stopping and Model_Checkpoints are optional parameters\n",
        "# Early Stopping is to stop the training based on certain condition set by the user\n",
        "# Model Checkpoint is to save a model in a directory based on certain conditions so that it can be used later for Transfer Learning or avoiding retraining\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=50, mode='min'), ModelCheckpoint('Saved_Model.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
        "history = Classification_Model.fit(X_Train, Y_Train, epochs=300, batch_size=64, verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "35/35 [==============================] - 11s 80ms/step - loss: 1.8749 - mse: 0.1016 - val_loss: 2.3296 - val_mse: 0.1189\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.32964, saving model to Saved_Model.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.8039 - mse: 0.0500 - val_loss: 2.3213 - val_mse: 0.0853\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.32964 to 2.32125, saving model to Saved_Model.h5\n",
            "Epoch 3/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.5287 - mse: 0.0299 - val_loss: 1.1925 - val_mse: 0.0626\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.32125 to 1.19246, saving model to Saved_Model.h5\n",
            "Epoch 4/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.3574 - mse: 0.0203 - val_loss: 1.2752 - val_mse: 0.0574\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.19246\n",
            "Epoch 5/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.2556 - mse: 0.0139 - val_loss: 1.2416 - val_mse: 0.0672\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.19246\n",
            "Epoch 6/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.3720 - mse: 0.0203 - val_loss: 1.8171 - val_mse: 0.0566\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.19246\n",
            "Epoch 7/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.2133 - mse: 0.0113 - val_loss: 2.5521 - val_mse: 0.0961\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.19246\n",
            "Epoch 8/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.1758 - mse: 0.0101 - val_loss: 0.6943 - val_mse: 0.0318\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.19246 to 0.69434, saving model to Saved_Model.h5\n",
            "Epoch 9/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.2191 - mse: 0.0111 - val_loss: 1.7042 - val_mse: 0.0591\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69434\n",
            "Epoch 10/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.1379 - mse: 0.0090 - val_loss: 0.9837 - val_mse: 0.0392\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69434\n",
            "Epoch 11/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.1365 - mse: 0.0093 - val_loss: 0.9664 - val_mse: 0.0360\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69434\n",
            "Epoch 12/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.1578 - mse: 0.0102 - val_loss: 0.4441 - val_mse: 0.0196\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.69434 to 0.44412, saving model to Saved_Model.h5\n",
            "Epoch 13/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0798 - mse: 0.0055 - val_loss: 0.6629 - val_mse: 0.0241\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.44412\n",
            "Epoch 14/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.1271 - mse: 0.0069 - val_loss: 0.5911 - val_mse: 0.0242\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.44412\n",
            "Epoch 15/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0725 - mse: 0.0041 - val_loss: 0.3077 - val_mse: 0.0142\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.44412 to 0.30768, saving model to Saved_Model.h5\n",
            "Epoch 16/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.1177 - mse: 0.0080 - val_loss: 0.5627 - val_mse: 0.0217\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.30768\n",
            "Epoch 17/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0904 - mse: 0.0058 - val_loss: 0.3466 - val_mse: 0.0177\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.30768\n",
            "Epoch 18/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.1047 - mse: 0.0068 - val_loss: 0.3464 - val_mse: 0.0143\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.30768\n",
            "Epoch 19/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0513 - mse: 0.0036 - val_loss: 0.2545 - val_mse: 0.0117\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.30768 to 0.25454, saving model to Saved_Model.h5\n",
            "Epoch 20/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0243 - mse: 0.0017 - val_loss: 0.2161 - val_mse: 0.0106\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.25454 to 0.21613, saving model to Saved_Model.h5\n",
            "Epoch 21/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0526 - mse: 0.0035 - val_loss: 0.3073 - val_mse: 0.0109\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.21613\n",
            "Epoch 22/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0638 - mse: 0.0037 - val_loss: 0.2442 - val_mse: 0.0110\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.21613\n",
            "Epoch 23/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0487 - mse: 0.0031 - val_loss: 0.1982 - val_mse: 0.0089\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.21613 to 0.19816, saving model to Saved_Model.h5\n",
            "Epoch 24/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0172 - mse: 0.0012 - val_loss: 0.1775 - val_mse: 0.0068\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.19816 to 0.17748, saving model to Saved_Model.h5\n",
            "Epoch 25/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0086 - mse: 5.1518e-04 - val_loss: 0.2700 - val_mse: 0.0094\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.17748\n",
            "Epoch 26/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0198 - mse: 0.0015 - val_loss: 0.3008 - val_mse: 0.0099\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.17748\n",
            "Epoch 27/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0371 - mse: 0.0026 - val_loss: 0.2127 - val_mse: 0.0106\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.17748\n",
            "Epoch 28/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0451 - mse: 0.0027 - val_loss: 0.3046 - val_mse: 0.0124\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.17748\n",
            "Epoch 29/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0093 - mse: 5.9551e-04 - val_loss: 0.3337 - val_mse: 0.0151\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.17748\n",
            "Epoch 30/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0322 - mse: 0.0023 - val_loss: 0.2763 - val_mse: 0.0119\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.17748\n",
            "Epoch 31/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0266 - mse: 0.0019 - val_loss: 0.1759 - val_mse: 0.0068\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.17748 to 0.17593, saving model to Saved_Model.h5\n",
            "Epoch 32/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0444 - mse: 0.0025 - val_loss: 0.2139 - val_mse: 0.0084\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.17593\n",
            "Epoch 33/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0180 - mse: 0.0012 - val_loss: 0.1935 - val_mse: 0.0070\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.17593\n",
            "Epoch 34/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0197 - mse: 9.6946e-04 - val_loss: 0.2203 - val_mse: 0.0086\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.17593\n",
            "Epoch 35/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0232 - mse: 0.0011 - val_loss: 0.2168 - val_mse: 0.0091\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.17593\n",
            "Epoch 36/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0108 - mse: 6.0407e-04 - val_loss: 0.2263 - val_mse: 0.0086\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.17593\n",
            "Epoch 37/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0032 - mse: 1.1848e-04 - val_loss: 0.2540 - val_mse: 0.0087\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.17593\n",
            "Epoch 38/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0015 - mse: 5.5818e-05 - val_loss: 0.3224 - val_mse: 0.0115\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.17593\n",
            "Epoch 39/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0637 - mse: 0.0042 - val_loss: 0.3054 - val_mse: 0.0104\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.17593\n",
            "Epoch 40/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0361 - mse: 0.0024 - val_loss: 0.2165 - val_mse: 0.0094\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.17593\n",
            "Epoch 41/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.1023 - mse: 0.0074 - val_loss: 0.1859 - val_mse: 0.0077\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.17593\n",
            "Epoch 42/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0438 - mse: 0.0023 - val_loss: 0.2470 - val_mse: 0.0092\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.17593\n",
            "Epoch 43/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0181 - mse: 0.0013 - val_loss: 0.1731 - val_mse: 0.0062\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.17593 to 0.17312, saving model to Saved_Model.h5\n",
            "Epoch 44/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0053 - mse: 2.8507e-04 - val_loss: 0.1583 - val_mse: 0.0055\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.17312 to 0.15830, saving model to Saved_Model.h5\n",
            "Epoch 45/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0266 - mse: 0.0017 - val_loss: 0.1754 - val_mse: 0.0069\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.15830\n",
            "Epoch 46/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0141 - mse: 9.2437e-04 - val_loss: 0.1897 - val_mse: 0.0071\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.15830\n",
            "Epoch 47/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0138 - mse: 0.0010 - val_loss: 0.1971 - val_mse: 0.0072\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.15830\n",
            "Epoch 48/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0582 - mse: 0.0032 - val_loss: 0.2601 - val_mse: 0.0091\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.15830\n",
            "Epoch 49/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0186 - mse: 0.0010 - val_loss: 0.3303 - val_mse: 0.0108\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.15830\n",
            "Epoch 50/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0190 - mse: 0.0011 - val_loss: 0.2385 - val_mse: 0.0077\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.15830\n",
            "Epoch 51/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0530 - mse: 0.0026 - val_loss: 0.2161 - val_mse: 0.0090\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.15830\n",
            "Epoch 52/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0219 - mse: 0.0014 - val_loss: 0.2760 - val_mse: 0.0093\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.15830\n",
            "Epoch 53/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0079 - mse: 4.6860e-04 - val_loss: 0.2804 - val_mse: 0.0092\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.15830\n",
            "Epoch 54/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0533 - mse: 0.0033 - val_loss: 0.5499 - val_mse: 0.0235\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.15830\n",
            "Epoch 55/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.1906 - mse: 0.0113 - val_loss: 0.2383 - val_mse: 0.0112\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.15830\n",
            "Epoch 56/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0196 - mse: 0.0011 - val_loss: 0.2806 - val_mse: 0.0103\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.15830\n",
            "Epoch 57/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0213 - mse: 0.0014 - val_loss: 0.3410 - val_mse: 0.0132\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.15830\n",
            "Epoch 58/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0194 - mse: 0.0012 - val_loss: 0.1832 - val_mse: 0.0069\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.15830\n",
            "Epoch 59/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0129 - mse: 6.3426e-04 - val_loss: 0.1872 - val_mse: 0.0081\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.15830\n",
            "Epoch 60/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0353 - mse: 0.0021 - val_loss: 0.1694 - val_mse: 0.0076\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.15830\n",
            "Epoch 61/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0665 - mse: 0.0046 - val_loss: 0.1467 - val_mse: 0.0077\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.15830 to 0.14669, saving model to Saved_Model.h5\n",
            "Epoch 62/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0044 - mse: 1.3996e-04 - val_loss: 0.1988 - val_mse: 0.0079\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.14669\n",
            "Epoch 63/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0040 - mse: 2.6818e-04 - val_loss: 0.2315 - val_mse: 0.0086\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.14669\n",
            "Epoch 64/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0027 - mse: 1.5140e-04 - val_loss: 0.2749 - val_mse: 0.0096\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.14669\n",
            "Epoch 65/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0143 - mse: 7.6663e-04 - val_loss: 0.1943 - val_mse: 0.0072\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.14669\n",
            "Epoch 66/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0127 - mse: 8.4622e-04 - val_loss: 0.2747 - val_mse: 0.0107\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.14669\n",
            "Epoch 67/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0099 - mse: 6.7853e-04 - val_loss: 0.1910 - val_mse: 0.0074\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.14669\n",
            "Epoch 68/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0319 - mse: 0.0018 - val_loss: 0.2631 - val_mse: 0.0094\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.14669\n",
            "Epoch 69/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0283 - mse: 0.0016 - val_loss: 0.4613 - val_mse: 0.0212\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.14669\n",
            "Epoch 70/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0316 - mse: 0.0019 - val_loss: 0.2286 - val_mse: 0.0089\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.14669\n",
            "Epoch 71/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0024 - mse: 8.2585e-05 - val_loss: 0.2531 - val_mse: 0.0085\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.14669\n",
            "Epoch 72/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0172 - mse: 0.0012 - val_loss: 0.1919 - val_mse: 0.0063\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.14669\n",
            "Epoch 73/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0018 - mse: 4.9879e-05 - val_loss: 0.2045 - val_mse: 0.0062\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.14669\n",
            "Epoch 74/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0675 - mse: 0.0042 - val_loss: 0.2288 - val_mse: 0.0063\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.14669\n",
            "Epoch 75/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0124 - mse: 5.9832e-04 - val_loss: 0.2302 - val_mse: 0.0085\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.14669\n",
            "Epoch 76/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0320 - mse: 0.0021 - val_loss: 6.7095 - val_mse: 0.1365\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.14669\n",
            "Epoch 77/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0339 - mse: 0.0016 - val_loss: 0.1713 - val_mse: 0.0060\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.14669\n",
            "Epoch 78/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0225 - mse: 0.0014 - val_loss: 0.6943 - val_mse: 0.0428\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.14669\n",
            "Epoch 79/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.4888 - mse: 0.0247 - val_loss: 6.3768 - val_mse: 0.1256\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.14669\n",
            "Epoch 80/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0820 - mse: 0.0038 - val_loss: 1.1256 - val_mse: 0.0305\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.14669\n",
            "Epoch 81/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0274 - mse: 0.0016 - val_loss: 0.1911 - val_mse: 0.0073\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.14669\n",
            "Epoch 82/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0076 - mse: 3.6599e-04 - val_loss: 0.2651 - val_mse: 0.0095\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.14669\n",
            "Epoch 83/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0103 - mse: 5.5088e-04 - val_loss: 0.1934 - val_mse: 0.0073\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.14669\n",
            "Epoch 84/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.1680 - mse: 0.0078 - val_loss: 0.5416 - val_mse: 0.0293\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.14669\n",
            "Epoch 85/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0409 - mse: 0.0026 - val_loss: 0.2083 - val_mse: 0.0093\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.14669\n",
            "Epoch 86/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0354 - mse: 0.0017 - val_loss: 0.1845 - val_mse: 0.0085\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.14669\n",
            "Epoch 87/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0075 - mse: 3.9110e-04 - val_loss: 0.1850 - val_mse: 0.0082\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.14669\n",
            "Epoch 88/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0033 - mse: 1.7235e-04 - val_loss: 0.1670 - val_mse: 0.0070\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.14669\n",
            "Epoch 89/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0017 - mse: 6.0749e-05 - val_loss: 0.1749 - val_mse: 0.0066\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.14669\n",
            "Epoch 90/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0145 - mse: 9.2028e-04 - val_loss: 0.7662 - val_mse: 0.0479\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.14669\n",
            "Epoch 91/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0209 - mse: 0.0013 - val_loss: 0.1501 - val_mse: 0.0052\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.14669\n",
            "Epoch 92/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0156 - mse: 9.6415e-04 - val_loss: 0.1668 - val_mse: 0.0073\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.14669\n",
            "Epoch 93/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0073 - mse: 3.4480e-04 - val_loss: 0.2547 - val_mse: 0.0156\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.14669\n",
            "Epoch 94/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.3470 - mse: 0.0212 - val_loss: 2.4393 - val_mse: 0.0685\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.14669\n",
            "Epoch 95/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0755 - mse: 0.0049 - val_loss: 0.3010 - val_mse: 0.0109\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.14669\n",
            "Epoch 96/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0260 - mse: 0.0016 - val_loss: 0.1855 - val_mse: 0.0068\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.14669\n",
            "Epoch 97/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0056 - mse: 2.3140e-04 - val_loss: 0.1869 - val_mse: 0.0075\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.14669\n",
            "Epoch 98/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0103 - mse: 5.4475e-04 - val_loss: 0.1883 - val_mse: 0.0059\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.14669\n",
            "Epoch 99/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0026 - mse: 5.5520e-05 - val_loss: 0.1732 - val_mse: 0.0059\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.14669\n",
            "Epoch 100/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0103 - mse: 5.6993e-04 - val_loss: 0.1822 - val_mse: 0.0055\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.14669\n",
            "Epoch 101/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0128 - mse: 7.5928e-04 - val_loss: 0.1962 - val_mse: 0.0057\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.14669\n",
            "Epoch 102/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0032 - mse: 1.3371e-04 - val_loss: 0.1606 - val_mse: 0.0056\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.14669\n",
            "Epoch 103/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0061 - mse: 3.7511e-04 - val_loss: 0.1401 - val_mse: 0.0051\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.14669 to 0.14006, saving model to Saved_Model.h5\n",
            "Epoch 104/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0117 - mse: 8.4010e-04 - val_loss: 0.1939 - val_mse: 0.0057\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.14006\n",
            "Epoch 105/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0052 - mse: 3.3272e-04 - val_loss: 0.1973 - val_mse: 0.0065\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.14006\n",
            "Epoch 106/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0069 - mse: 3.4273e-04 - val_loss: 0.1637 - val_mse: 0.0051\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.14006\n",
            "Epoch 107/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0374 - mse: 0.0025 - val_loss: 0.2384 - val_mse: 0.0067\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.14006\n",
            "Epoch 108/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0118 - mse: 7.0438e-04 - val_loss: 0.1337 - val_mse: 0.0046\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.14006 to 0.13368, saving model to Saved_Model.h5\n",
            "Epoch 109/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0032 - mse: 1.0646e-04 - val_loss: 0.2124 - val_mse: 0.0065\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.13368\n",
            "Epoch 110/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0160 - mse: 5.8207e-04 - val_loss: 0.1862 - val_mse: 0.0074\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.13368\n",
            "Epoch 111/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0114 - mse: 7.5860e-04 - val_loss: 0.1272 - val_mse: 0.0045\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.13368 to 0.12721, saving model to Saved_Model.h5\n",
            "Epoch 112/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0019 - mse: 8.2605e-05 - val_loss: 0.1352 - val_mse: 0.0050\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.12721\n",
            "Epoch 113/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0066 - mse: 4.5994e-04 - val_loss: 0.1602 - val_mse: 0.0058\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.12721\n",
            "Epoch 114/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0014 - mse: 5.1305e-05 - val_loss: 0.1299 - val_mse: 0.0047\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.12721\n",
            "Epoch 115/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 4.3435e-04 - mse: 5.6876e-07 - val_loss: 0.1393 - val_mse: 0.0049\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.12721\n",
            "Epoch 116/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 7.4301e-04 - mse: 1.5447e-05 - val_loss: 0.1668 - val_mse: 0.0055\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.12721\n",
            "Epoch 117/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.1014 - mse: 0.0065 - val_loss: 0.1866 - val_mse: 0.0071\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.12721\n",
            "Epoch 118/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0248 - mse: 0.0011 - val_loss: 0.1500 - val_mse: 0.0053\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.12721\n",
            "Epoch 119/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0146 - mse: 7.7187e-04 - val_loss: 0.1589 - val_mse: 0.0057\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.12721\n",
            "Epoch 120/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0054 - mse: 3.2974e-04 - val_loss: 0.1364 - val_mse: 0.0057\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.12721\n",
            "Epoch 121/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0013 - mse: 3.3402e-05 - val_loss: 0.1581 - val_mse: 0.0059\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.12721\n",
            "Epoch 122/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 9.1091e-04 - mse: 3.5643e-05 - val_loss: 0.1428 - val_mse: 0.0052\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.12721\n",
            "Epoch 123/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.1002 - mse: 0.0047 - val_loss: 0.1124 - val_mse: 0.0057\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.12721 to 0.11235, saving model to Saved_Model.h5\n",
            "Epoch 124/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0193 - mse: 0.0011 - val_loss: 0.1295 - val_mse: 0.0067\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.11235\n",
            "Epoch 125/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0075 - mse: 3.1120e-04 - val_loss: 0.1178 - val_mse: 0.0055\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.11235\n",
            "Epoch 126/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0047 - mse: 2.2218e-04 - val_loss: 0.1332 - val_mse: 0.0061\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.11235\n",
            "Epoch 127/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0013 - mse: 1.9619e-05 - val_loss: 0.1486 - val_mse: 0.0062\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.11235\n",
            "Epoch 128/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 4.8319e-04 - mse: 2.7408e-06 - val_loss: 0.1368 - val_mse: 0.0054\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.11235\n",
            "Epoch 129/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 3.9779e-04 - mse: 3.1860e-06 - val_loss: 0.1318 - val_mse: 0.0051\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.11235\n",
            "Epoch 130/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 4.9361e-04 - mse: 1.4707e-05 - val_loss: 0.1301 - val_mse: 0.0051\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.11235\n",
            "Epoch 131/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 2.9726e-04 - mse: 5.9730e-06 - val_loss: 0.1593 - val_mse: 0.0063\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.11235\n",
            "Epoch 132/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0072 - mse: 3.6253e-04 - val_loss: 0.4031 - val_mse: 0.0269\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.11235\n",
            "Epoch 133/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0125 - mse: 8.2931e-04 - val_loss: 0.1506 - val_mse: 0.0069\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.11235\n",
            "Epoch 134/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0322 - mse: 0.0017 - val_loss: 0.3477 - val_mse: 0.0132\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.11235\n",
            "Epoch 135/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0253 - mse: 0.0018 - val_loss: 0.1425 - val_mse: 0.0059\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.11235\n",
            "Epoch 136/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0038 - mse: 2.2894e-04 - val_loss: 0.1202 - val_mse: 0.0056\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.11235\n",
            "Epoch 137/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 6.7920e-04 - mse: 1.8169e-05 - val_loss: 0.0995 - val_mse: 0.0043\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.11235 to 0.09945, saving model to Saved_Model.h5\n",
            "Epoch 138/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0029 - mse: 2.3090e-04 - val_loss: 0.1244 - val_mse: 0.0050\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.09945\n",
            "Epoch 139/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0021 - mse: 1.4994e-04 - val_loss: 0.0953 - val_mse: 0.0042\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.09945 to 0.09533, saving model to Saved_Model.h5\n",
            "Epoch 140/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0071 - mse: 3.7607e-04 - val_loss: 0.0900 - val_mse: 0.0043\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.09533 to 0.09001, saving model to Saved_Model.h5\n",
            "Epoch 141/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0017 - mse: 8.4626e-05 - val_loss: 0.1287 - val_mse: 0.0060\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.09001\n",
            "Epoch 142/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0025 - mse: 1.2469e-04 - val_loss: 0.2069 - val_mse: 0.0078\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.09001\n",
            "Epoch 143/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.1616 - mse: 0.0084 - val_loss: 0.2269 - val_mse: 0.0107\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.09001\n",
            "Epoch 144/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0501 - mse: 0.0030 - val_loss: 0.1208 - val_mse: 0.0048\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.09001\n",
            "Epoch 145/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0240 - mse: 0.0016 - val_loss: 0.2471 - val_mse: 0.0094\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.09001\n",
            "Epoch 146/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0261 - mse: 0.0012 - val_loss: 0.0985 - val_mse: 0.0040\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.09001\n",
            "Epoch 147/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0080 - mse: 2.5142e-04 - val_loss: 0.0978 - val_mse: 0.0032\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.09001\n",
            "Epoch 148/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0015 - mse: 3.6020e-05 - val_loss: 0.1211 - val_mse: 0.0049\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.09001\n",
            "Epoch 149/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0012 - mse: 1.6190e-05 - val_loss: 0.1247 - val_mse: 0.0049\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.09001\n",
            "Epoch 150/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 7.9480e-04 - mse: 5.0430e-06 - val_loss: 0.1276 - val_mse: 0.0048\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.09001\n",
            "Epoch 151/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 9.6689e-04 - mse: 4.2518e-05 - val_loss: 0.1147 - val_mse: 0.0041\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.09001\n",
            "Epoch 152/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0747 - mse: 0.0040 - val_loss: 0.1857 - val_mse: 0.0074\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.09001\n",
            "Epoch 153/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0209 - mse: 0.0011 - val_loss: 0.1994 - val_mse: 0.0077\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.09001\n",
            "Epoch 154/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0123 - mse: 8.0965e-04 - val_loss: 0.1102 - val_mse: 0.0046\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.09001\n",
            "Epoch 155/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0011 - mse: 1.2827e-05 - val_loss: 0.1364 - val_mse: 0.0055\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.09001\n",
            "Epoch 156/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0016 - mse: 6.7822e-05 - val_loss: 0.1422 - val_mse: 0.0056\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.09001\n",
            "Epoch 157/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0166 - mse: 9.4260e-04 - val_loss: 0.1496 - val_mse: 0.0075\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.09001\n",
            "Epoch 158/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0077 - mse: 5.6003e-04 - val_loss: 0.0946 - val_mse: 0.0041\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.09001\n",
            "Epoch 159/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0053 - mse: 3.6418e-04 - val_loss: 0.1649 - val_mse: 0.0054\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.09001\n",
            "Epoch 160/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0044 - mse: 3.1935e-04 - val_loss: 0.1077 - val_mse: 0.0044\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.09001\n",
            "Epoch 161/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 8.4733e-04 - mse: 3.3259e-05 - val_loss: 0.1343 - val_mse: 0.0048\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.09001\n",
            "Epoch 162/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0655 - mse: 0.0038 - val_loss: 0.1594 - val_mse: 0.0071\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.09001\n",
            "Epoch 163/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0414 - mse: 0.0024 - val_loss: 9.7785 - val_mse: 0.0998\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.09001\n",
            "Epoch 164/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0062 - mse: 3.1873e-04 - val_loss: 0.3041 - val_mse: 0.0153\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.09001\n",
            "Epoch 165/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0034 - mse: 1.6906e-04 - val_loss: 0.1090 - val_mse: 0.0043\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.09001\n",
            "Epoch 166/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0010 - mse: 2.8054e-05 - val_loss: 0.1401 - val_mse: 0.0054\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.09001\n",
            "Epoch 167/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0076 - mse: 2.2247e-04 - val_loss: 0.1302 - val_mse: 0.0055\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.09001\n",
            "Epoch 168/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0045 - mse: 2.4694e-04 - val_loss: 0.1339 - val_mse: 0.0056\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.09001\n",
            "Epoch 169/300\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0012 - mse: 5.8193e-05 - val_loss: 0.1096 - val_mse: 0.0045\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.09001\n",
            "Epoch 170/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 8.2984e-04 - mse: 3.6808e-05 - val_loss: 0.1127 - val_mse: 0.0043\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.09001\n",
            "Epoch 171/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0043 - mse: 1.2170e-04 - val_loss: 0.1227 - val_mse: 0.0048\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.09001\n",
            "Epoch 172/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0013 - mse: 3.3944e-05 - val_loss: 0.1256 - val_mse: 0.0051\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.09001\n",
            "Epoch 173/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 8.6442e-04 - mse: 5.5094e-05 - val_loss: 0.1596 - val_mse: 0.0057\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.09001\n",
            "Epoch 174/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 1.7242e-04 - mse: 4.6340e-07 - val_loss: 0.1531 - val_mse: 0.0055\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.09001\n",
            "Epoch 175/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 2.1487e-04 - mse: 5.8305e-06 - val_loss: 0.1522 - val_mse: 0.0057\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.09001\n",
            "Epoch 176/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0947 - mse: 0.0054 - val_loss: 1.0345 - val_mse: 0.0699\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.09001\n",
            "Epoch 177/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.1485 - mse: 0.0102 - val_loss: 17.1162 - val_mse: 0.0657\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.09001\n",
            "Epoch 178/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0672 - mse: 0.0037 - val_loss: 0.1539 - val_mse: 0.0062\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.09001\n",
            "Epoch 179/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0050 - mse: 1.1289e-04 - val_loss: 0.1698 - val_mse: 0.0057\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.09001\n",
            "Epoch 180/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0041 - mse: 2.2817e-04 - val_loss: 0.1673 - val_mse: 0.0057\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.09001\n",
            "Epoch 181/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0072 - mse: 4.4362e-04 - val_loss: 0.1862 - val_mse: 0.0061\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.09001\n",
            "Epoch 182/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0172 - mse: 9.2906e-04 - val_loss: 0.0930 - val_mse: 0.0047\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.09001\n",
            "Epoch 183/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0029 - mse: 9.4459e-05 - val_loss: 0.1710 - val_mse: 0.0062\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.09001\n",
            "Epoch 184/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0086 - mse: 2.8436e-04 - val_loss: 0.1361 - val_mse: 0.0054\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.09001\n",
            "Epoch 185/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0102 - mse: 5.5749e-04 - val_loss: 0.1564 - val_mse: 0.0063\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.09001\n",
            "Epoch 186/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0742 - mse: 0.0046 - val_loss: 0.0927 - val_mse: 0.0039\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.09001\n",
            "Epoch 187/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0414 - mse: 0.0026 - val_loss: 0.0745 - val_mse: 0.0034\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.09001 to 0.07449, saving model to Saved_Model.h5\n",
            "Epoch 188/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0082 - mse: 5.3397e-04 - val_loss: 0.0764 - val_mse: 0.0031\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.07449\n",
            "Epoch 189/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0042 - mse: 1.7468e-04 - val_loss: 0.0856 - val_mse: 0.0027\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.07449\n",
            "Epoch 190/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.4611 - mse: 0.0214 - val_loss: 3.6932 - val_mse: 0.0932\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.07449\n",
            "Epoch 191/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0497 - mse: 0.0027 - val_loss: 0.2110 - val_mse: 0.0125\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.07449\n",
            "Epoch 192/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0377 - mse: 0.0022 - val_loss: 0.1358 - val_mse: 0.0069\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.07449\n",
            "Epoch 193/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0063 - mse: 3.6519e-04 - val_loss: 0.0800 - val_mse: 0.0033\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.07449\n",
            "Epoch 194/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0062 - mse: 3.5125e-04 - val_loss: 0.0791 - val_mse: 0.0027\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.07449\n",
            "Epoch 195/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0010 - mse: 4.7304e-06 - val_loss: 0.0808 - val_mse: 0.0029\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.07449\n",
            "Epoch 196/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 5.7785e-04 - mse: 4.6916e-06 - val_loss: 0.0841 - val_mse: 0.0031\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.07449\n",
            "Epoch 197/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0211 - mse: 0.0012 - val_loss: 0.1025 - val_mse: 0.0038\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.07449\n",
            "Epoch 198/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0085 - mse: 5.4787e-04 - val_loss: 0.0828 - val_mse: 0.0028\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.07449\n",
            "Epoch 199/300\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0252 - mse: 0.0014 - val_loss: 0.1910 - val_mse: 0.0074\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.07449\n",
            "Epoch 200/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0115 - mse: 6.1954e-04 - val_loss: 0.1241 - val_mse: 0.0046\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.07449\n",
            "Epoch 201/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0152 - mse: 8.0973e-04 - val_loss: 0.1187 - val_mse: 0.0053\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.07449\n",
            "Epoch 202/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0459 - mse: 0.0027 - val_loss: 0.2105 - val_mse: 0.0100\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.07449\n",
            "Epoch 203/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0280 - mse: 0.0019 - val_loss: 0.1103 - val_mse: 0.0042\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.07449\n",
            "Epoch 204/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0400 - mse: 0.0014 - val_loss: 0.0966 - val_mse: 0.0035\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.07449\n",
            "Epoch 205/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0107 - mse: 5.5160e-04 - val_loss: 0.0807 - val_mse: 0.0032\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.07449\n",
            "Epoch 206/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0026 - mse: 7.9938e-05 - val_loss: 0.0845 - val_mse: 0.0034\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.07449\n",
            "Epoch 207/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 8.8179e-04 - mse: 1.8901e-05 - val_loss: 0.0875 - val_mse: 0.0034\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.07449\n",
            "Epoch 208/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0075 - mse: 5.1040e-04 - val_loss: 0.1443 - val_mse: 0.0067\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.07449\n",
            "Epoch 209/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0019 - mse: 6.0309e-05 - val_loss: 0.0872 - val_mse: 0.0033\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.07449\n",
            "Epoch 210/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0018 - mse: 1.1039e-04 - val_loss: 0.1258 - val_mse: 0.0056\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.07449\n",
            "Epoch 211/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 9.1251e-04 - mse: 9.1020e-06 - val_loss: 0.1118 - val_mse: 0.0051\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.07449\n",
            "Epoch 212/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 4.8255e-04 - mse: 2.9503e-06 - val_loss: 0.1035 - val_mse: 0.0046\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.07449\n",
            "Epoch 213/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 4.6404e-04 - mse: 3.6062e-06 - val_loss: 0.0882 - val_mse: 0.0035\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.07449\n",
            "Epoch 214/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 3.1281e-04 - mse: 2.4986e-06 - val_loss: 0.0858 - val_mse: 0.0036\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.07449\n",
            "Epoch 215/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 2.5786e-04 - mse: 1.0976e-06 - val_loss: 0.0890 - val_mse: 0.0036\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.07449\n",
            "Epoch 216/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 1.9193e-04 - mse: 8.1918e-07 - val_loss: 0.0905 - val_mse: 0.0035\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.07449\n",
            "Epoch 217/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0016 - mse: 1.1154e-04 - val_loss: 0.0837 - val_mse: 0.0030\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.07449\n",
            "Epoch 218/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0020 - mse: 1.3133e-04 - val_loss: 0.1013 - val_mse: 0.0038\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.07449\n",
            "Epoch 219/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0097 - mse: 6.3460e-04 - val_loss: 0.0858 - val_mse: 0.0029\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.07449\n",
            "Epoch 220/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0046 - mse: 2.7161e-04 - val_loss: 0.0759 - val_mse: 0.0022\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.07449\n",
            "Epoch 221/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0017 - mse: 1.2228e-04 - val_loss: 0.0782 - val_mse: 0.0026\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.07449\n",
            "Epoch 222/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0033 - mse: 2.1215e-04 - val_loss: 0.0736 - val_mse: 0.0025\n",
            "\n",
            "Epoch 00222: val_loss improved from 0.07449 to 0.07365, saving model to Saved_Model.h5\n",
            "Epoch 223/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0036 - mse: 2.2047e-04 - val_loss: 0.0723 - val_mse: 0.0027\n",
            "\n",
            "Epoch 00223: val_loss improved from 0.07365 to 0.07232, saving model to Saved_Model.h5\n",
            "Epoch 224/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0045 - mse: 1.9210e-04 - val_loss: 0.0725 - val_mse: 0.0027\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.07232\n",
            "Epoch 225/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0389 - mse: 0.0021 - val_loss: 0.1821 - val_mse: 0.0080\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.07232\n",
            "Epoch 226/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0183 - mse: 8.8031e-04 - val_loss: 0.0894 - val_mse: 0.0039\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.07232\n",
            "Epoch 227/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0036 - mse: 1.4321e-04 - val_loss: 0.0871 - val_mse: 0.0031\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.07232\n",
            "Epoch 228/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0185 - mse: 9.7349e-04 - val_loss: 0.1160 - val_mse: 0.0042\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.07232\n",
            "Epoch 229/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.1396 - mse: 0.0082 - val_loss: 0.2170 - val_mse: 0.0070\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.07232\n",
            "Epoch 230/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0109 - mse: 5.3926e-04 - val_loss: 0.1359 - val_mse: 0.0045\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.07232\n",
            "Epoch 231/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0012 - mse: 2.2870e-05 - val_loss: 0.1185 - val_mse: 0.0040\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.07232\n",
            "Epoch 232/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 7.7308e-04 - mse: 1.5971e-05 - val_loss: 0.1131 - val_mse: 0.0039\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.07232\n",
            "Epoch 233/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 6.0837e-04 - mse: 1.3201e-05 - val_loss: 0.0944 - val_mse: 0.0035\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.07232\n",
            "Epoch 234/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0017 - mse: 4.3061e-05 - val_loss: 0.1394 - val_mse: 0.0041\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.07232\n",
            "Epoch 235/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0069 - mse: 4.3199e-04 - val_loss: 0.0896 - val_mse: 0.0043\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.07232\n",
            "Epoch 236/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0058 - mse: 3.9183e-04 - val_loss: 0.0806 - val_mse: 0.0036\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.07232\n",
            "Epoch 237/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0486 - mse: 0.0029 - val_loss: 0.1085 - val_mse: 0.0039\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.07232\n",
            "Epoch 238/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0154 - mse: 7.6229e-04 - val_loss: 0.0808 - val_mse: 0.0028\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.07232\n",
            "Epoch 239/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0110 - mse: 5.5715e-04 - val_loss: 0.0976 - val_mse: 0.0037\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.07232\n",
            "Epoch 240/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0217 - mse: 0.0014 - val_loss: 0.0860 - val_mse: 0.0033\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.07232\n",
            "Epoch 241/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0045 - mse: 1.9687e-04 - val_loss: 0.0932 - val_mse: 0.0038\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.07232\n",
            "Epoch 242/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0150 - mse: 0.0013 - val_loss: 0.0980 - val_mse: 0.0037\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.07232\n",
            "Epoch 243/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0116 - mse: 6.0080e-04 - val_loss: 0.1397 - val_mse: 0.0078\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.07232\n",
            "Epoch 244/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0307 - mse: 0.0020 - val_loss: 0.0923 - val_mse: 0.0033\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.07232\n",
            "Epoch 245/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0018 - mse: 7.2674e-05 - val_loss: 0.0844 - val_mse: 0.0033\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.07232\n",
            "Epoch 246/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0018 - mse: 7.1813e-05 - val_loss: 0.0982 - val_mse: 0.0035\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.07232\n",
            "Epoch 247/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 6.8904e-04 - mse: 7.6449e-06 - val_loss: 0.1036 - val_mse: 0.0038\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.07232\n",
            "Epoch 248/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 3.2167e-04 - mse: 7.2692e-07 - val_loss: 0.1063 - val_mse: 0.0039\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.07232\n",
            "Epoch 249/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 2.3292e-04 - mse: 2.8814e-07 - val_loss: 0.1076 - val_mse: 0.0039\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.07232\n",
            "Epoch 250/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 2.9675e-04 - mse: 1.3357e-06 - val_loss: 0.1088 - val_mse: 0.0039\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.07232\n",
            "Epoch 251/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0017 - mse: 6.8927e-05 - val_loss: 0.1191 - val_mse: 0.0043\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.07232\n",
            "Epoch 252/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0649 - mse: 0.0034 - val_loss: 1.7795 - val_mse: 0.1128\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.07232\n",
            "Epoch 253/300\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0651 - mse: 0.0039 - val_loss: 0.1154 - val_mse: 0.0043\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.07232\n",
            "Epoch 254/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0017 - mse: 2.9984e-05 - val_loss: 0.1166 - val_mse: 0.0043\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.07232\n",
            "Epoch 255/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0012 - mse: 1.0780e-05 - val_loss: 0.1189 - val_mse: 0.0045\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.07232\n",
            "Epoch 256/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 5.5133e-04 - mse: 7.3812e-07 - val_loss: 0.1173 - val_mse: 0.0043\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.07232\n",
            "Epoch 257/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 6.9331e-04 - mse: 1.2995e-05 - val_loss: 0.1276 - val_mse: 0.0047\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.07232\n",
            "Epoch 258/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0064 - mse: 3.0593e-04 - val_loss: 0.0988 - val_mse: 0.0035\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.07232\n",
            "Epoch 259/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0435 - mse: 0.0028 - val_loss: 0.0795 - val_mse: 0.0030\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.07232\n",
            "Epoch 260/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0181 - mse: 0.0010 - val_loss: 0.0850 - val_mse: 0.0033\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.07232\n",
            "Epoch 261/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0179 - mse: 0.0014 - val_loss: 0.1067 - val_mse: 0.0036\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.07232\n",
            "Epoch 262/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0036 - mse: 1.6881e-04 - val_loss: 0.1081 - val_mse: 0.0033\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.07232\n",
            "Epoch 263/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 9.9212e-04 - mse: 1.7403e-05 - val_loss: 0.1026 - val_mse: 0.0029\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.07232\n",
            "Epoch 264/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 3.0097e-04 - mse: 5.8645e-07 - val_loss: 0.1039 - val_mse: 0.0029\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.07232\n",
            "Epoch 265/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0015 - mse: 1.0301e-04 - val_loss: 0.1097 - val_mse: 0.0035\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.07232\n",
            "Epoch 266/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 8.6032e-04 - mse: 3.7376e-05 - val_loss: 0.0967 - val_mse: 0.0026\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.07232\n",
            "Epoch 267/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0098 - mse: 5.7141e-04 - val_loss: 0.1156 - val_mse: 0.0038\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.07232\n",
            "Epoch 268/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0015 - mse: 8.1676e-05 - val_loss: 0.1082 - val_mse: 0.0035\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.07232\n",
            "Epoch 269/300\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0308 - mse: 0.0019 - val_loss: 0.1476 - val_mse: 0.0068\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.07232\n",
            "Epoch 270/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0317 - mse: 0.0018 - val_loss: 0.0983 - val_mse: 0.0039\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.07232\n",
            "Epoch 271/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0033 - mse: 1.6758e-04 - val_loss: 0.1195 - val_mse: 0.0046\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.07232\n",
            "Epoch 272/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 5.3779e-04 - mse: 9.4315e-06 - val_loss: 0.1199 - val_mse: 0.0045\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.07232\n",
            "Epoch 273/300\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 3.6649e-04 - mse: 3.1899e-06 - val_loss: 0.1111 - val_mse: 0.0042\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.07232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2IuS_52qqrR"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2ofzKUWmmi2",
        "outputId": "abb26f0f-af2f-4f72-9137-5e39f26bd145"
      },
      "source": [
        "# Predictions from the Test Set from the Trained Model\n",
        "Predictions = Classification_Model.predict(X_Test, verbose=1)\n",
        "print(Predictions.shape)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 2s 8ms/step\n",
            "(685, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF5p4FJcqvBj"
      },
      "source": [
        "Error Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGa14mh9qvGL",
        "outputId": "191036bf-e0eb-49dc-f13f-e2f66ab55eca"
      },
      "source": [
        "# Error of the prediction, one of many evaluation metrics\n",
        "# Using Mean Absolute Error (MAE) in this case as a sample\n",
        "Error = mean_absolute_error(Y_Test, Predictions)\n",
        "print(f\"MAE: {Error}\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: 0.0032112621617213814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIIikrVK767j"
      },
      "source": [
        "History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "zTR3UK1e78Yl",
        "outputId": "208134f2-8e74-41f5-dcfd-713e3f5754c2"
      },
      "source": [
        "def history_plot(history):\n",
        "  # list all dictionaries in history\n",
        "  print(history.history.keys())\n",
        "  # summarize history for error\n",
        "  plt.figure(figsize=(12,10))\n",
        "  plt.subplot(2,1,1)\n",
        "  plt.plot(history.history['mse'])\n",
        "  plt.plot(history.history['val_mse'])\n",
        "  plt.title('Model Error Performance')\n",
        "  plt.ylabel('Error')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper right')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.figure(figsize=(12,10))\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper right')\n",
        "  plt.show()\n",
        "#\n",
        "history_plot(history)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbcxbcN4Ifs"
      },
      "source": [
        "Convert raw predictions into final, single-column, integer-based predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcNivlpZyFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869dc07c-63e1-4a9c-b2f1-b3496942d2be"
      },
      "source": [
        "prediction_shape = Predictions.shape\n",
        "prediction_length = prediction_shape[0]\n",
        "Y_Labels = np.zeros((prediction_length, 1))\n",
        "#\n",
        "for i in range(0, prediction_length):\n",
        "  prediction = Predictions[i]\n",
        "  x = np.where(prediction == np.max(prediction))\n",
        "  x = x[0]\n",
        "  x = x.astype('int')\n",
        "  Y_Labels[i] = x\n",
        "\n",
        "Y_Labels.shape"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(685, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYSh2K8s7utY"
      },
      "source": [
        "Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "_VxYdy8O3_pK",
        "outputId": "d75fb454-9a7b-4a3a-c7b0-8e1b297abfa9"
      },
      "source": [
        "Conf = {'y_Actual': Y_Test_.ravel(), 'y_Predicted': Y_Labels.ravel()}\n",
        "Conf = pd.DataFrame(data=Conf)\n",
        "df_Conf_Mat = pd.DataFrame(Conf, columns=['y_Actual', 'y_Predicted'])\n",
        "confusion_matrix = pd.crosstab(df_Conf_Mat['y_Actual'], df_Conf_Mat['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], normalize='columns')\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.heatmap(confusion_matrix, cmap='YlGnBu', annot=True)\n",
        "plt.title('Confusion Matrix', fontsize=20)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Confusion Matrix')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTm6TsJQ9aKK"
      },
      "source": [
        "Here it can be seen that risk level 5,6 and 7 could not be predicted well due to less number of instances."
      ]
    }
  ]
}